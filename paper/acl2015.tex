%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath, amssymb}

\usepackage[style=apa,bibstyle=apa,natbib=true,url=false,backend=biber]{biblatex}

\usepackage[american]{babel}
\DeclareLanguageMapping{american}{american-apa}

%% genitive citations
\newcommand{\citegen}[1]{\citeauthor{#1}'s \citeyear{#1}}
% \newcommand{\citep}[1]{\cite{#1}}
% \newcommand{\citet}[1]{\citeA{#1}}
% \newcommand{\citealt}[1]{\citeNP{#1}}

\addbibresource{cmcl.bib}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Unsupervised Psycholinguistics: Using Text-to-Text Realization to Model Language Production}

\author{%Jeremy Cole \\
%College of IST \\
%Pennsylvania State University \\
%University Park, PA, 16802, USA \\
  %{\tt jrcole@psu.edu} \\ \And
%David Reitter \\
%College of IST \\
%Pennsylvania State University \\
%University Park, PA, 16802, USA \\
  %{\tt reitter@psu.edu} \\
}

\date{April 30, 2015}


\begin{document}
\maketitle
\begin{abstract}
While text-to-text realization has achieved wide coverage of corpora, it offers little in the way of analyzing the underlying psychological and linguistic structure. While cognitive models of language production offer this, they can rarely reach the broad coverage of text-to-text realization systems. In this paper, we propose a system that will unite these two paradigms. This would allow the testing of psycholinguistic phenomenon on big data. We present initial findings on a few classical assumptions to investigate the validity of our methodology.  
\end{abstract}

\section{Introduction}
Text-to-Text realization has impressively demonstrated that the linguistic toolchain and a set of representations have reached the point where they can analyze the broad range of sentences found in corpora, and that they can recover sentences in generated output. While these tools are interesting to certain applications, they do not presently allow us to systematically validate the underlying components in terms of their psychological and linguistic plausibility. While systems that rely more explicitly on cognitive architectures and deep linguistic encoding have made great progress past simple example sentences, they have yet to reach the coverage of text-to-text realization. In this paper, we propose the start of a system that will merge these two paradigms  and allow for statements of cognitive plausibility to be tested on large data sets. The system is implemented using White's realization algorithm \citep{chart} in combination with the principles of ACT-R activation \citep{actr} and was tested using the Switchboard corpus. We investigated several key assumptions of ACT-R which we use to demonstrate that our methodology can be used to investigate language production in a mostly novel way.

Text-to-Text Realization (TTR) is the process of taking a body of text and converting it into semantic representations for that text, and then converting it back again. This in of itself may not seem particularly useful from an engineering standpoint: you already have what you're trying to produce! However, TTR consists of two interesting problems: (1) converting text into semantic representations and (2) producing language from those representations. Together, these two sub-problems allow for a completely unsupervised analysis. As the system only requires unannotated language data, it can be applied to extremely large datasets, which have so far mostly escaped the cognitive modeling of language production.

%There are three or four core linguistic components that are involved in these predictions. First, in order to parse the text into semantic annotations, both a grammar formalism and a system of semantic representations are necessary. The most popular grammar formalisms for current linguistic research seem to be Combinatory Categorial Grammar (CCG) \citep{ccg} and Tree-Adjoining Grammar (TAG) \citep{tag}. Grammar has implicit or explicit theoretical predictions for how people combine words into meaningful phrases. For semantic representations, a wide variety are used, such as $\lambda$ -calculus, first-order logic, Discourse Representation Semantics \cite{drs},  or Hybrid-Logic Dependency Semantics \citep{hlds1}. Semantic representation can also have theoretical predictions: how the ideas and knowledge we have is actually stored and combined in our minds. Realizing the text requires one additional component: something to choose which of the possible realizations of the semantics is the best. If the parsers and realizers were perfect, this would just be choosing between two semantically-equivalent statements. As it stands, realizations that seem to follow the correct syntactic structure and semantic meaning are ranked based on a language model, and the best one is output.
\section{Motivation}
Studying language production in an effective way thus far has been hard. Psycholinguistics generally relies on carefully controlled experiments; however, studying production in this way is difficult because participants are presented with a large degree of choice. For example, if you ask a subject to describe a picture without controlling what he or she will say, there are a (theoretically) infinite number of utterances he or she could produce. Due to the difficulty in collecting large amounts of data in such a setting, comparing a massive number of experimental conditions is not really feasible. \\
\indent For instance, consider an experimenter who wishes to contrast the choice of a Double Object construction (The boy threw his dog the ball) with a Prepositional Object construction (The boy threw the ball to his dog). If the participant is simply describing a picture, there is no guarantee he or she will even reference the dog! The primary strategy for countering this problem has been to provide the participant with the syntactic structure of the sentence \citep{incremental} or with the first few words of the utterance \citep{chinese-representation}. In the latter of these studies, the researchers still had to include an "Other" category for sentences that did not fit in any of the desired conditions. \\
\indent While these controls have been useful, they could have the side effect of muddling certain parts of the process: such as planning. In order to produce a sentence, it must first be planned. Investigating the planning process is difficult: many paradigms frequently used to investigate sentence comprehension, such as eye tracking or the visual world, seem much more difficult to apply. Despite the difficulty in designing experiments to measure planning, it is clearly of interest to researchers, as can be evidenced by ongoing debates. One such debate is to what degree is the process \emph{incremental}. \\
\indent For instance, with sentences that contain center-embedded clauses, is the end planned before or after the embedded clause? Consider the sentence "The dog that was chasing the cat that seemed to have rabies fell over." An incremental derivation would rely on \emph{surface order}: the actual order the words appear in the sentence. In other words, the speaker first plans "the dog", then "that was chasing the cat", then "that seemed to have rabies" and lastly "fell over." A less incremental derivation might be to plan "fell over" immediately after "the dog", and then to add the clauses later. \\
\indent Some researchers seem to take it as a given that language processes are radically incremental (especially with regards to comprehension) \citep{tag1}\citep{radical}. However, the evidence (especially for production) is not so clear \citep{incremental}. Due to the seeming difficulty in solving this debates such as these with more experimentation, we suggest turning to computational cognitive modeling.\\ 
\indent Computational cognitive models of language production are not novel in of themselves \citep{reitter2011syntacticpriming}. However, to some extent, they suffer from the same problem as experimentation. The breadth of linguistic diversity falls short of the sentences that any experimenters can plan for. To solve these problems, we need computational cognitive models with the wide coverage of text-to-text realization systems. We believe our model is a step in this direction.

\section{Background}
%Discuss in more detail the mechanisms used (specifically the realization algorithm, HLDS, the parser, CCG, ACT-R)
Our work leans heavily on previous work done by Michael White in text-to-text realization, which has been elaborated on and improved across several papers \citep{chart} \citep{chart-dep} \citep{chart-dlf}. We will briefly discuss the set of tools that the system uses to perform our experiments.

\subsection{Grammar}
A grammar, simply put, is a method to combine symbols until reaching a terminal state. Human language seems to be aptly modeled by minimally context-sensitive grammars: Context-Free Grammars lack the necessary descriptive power, while Context-Sensitive Grammars are too slow \citep{convergence}.

Combinatory Categorial Grammar (CCG), developed by Mark Steedman, is a minimally context-sensitive grammar in the family of categorial grammars \citep{ccg}. It seeks to model language with many syntactic types but only a few combinatory rules. For our purposes, CCG works well because it allows for various derivations of the same sentence, allowing planning processes to be modeled.  

\subsection{Semantics}
Semantic representations, such as First-Order Logic, have a long history in the field of knowledge representation. As the knowledge we wished to represent became more complicated, so did the systems. Hybrid-Logic Dependency Semantics (HLDS) is one such system \citep{hlds1}.

Baldridge et al summarized a few key points for why HLDS is preferable over other semantic representations for language. In short, it is expressive, flexible, computationally feasible, and can be coupled with a grammar formalism. Essentially, HLDS and CCG can be written as a single representation, which makes it ideal for text-to-text generation systems.

\subsection{Parser}
The goal of the parser is to take an English sentence input in the Penn Treebank format \citep{needed?} and convert it into an HLDS representation. To accomplish this, we used the OpenCCG parser \cite{not_sure_what} which in turn was based on the OpenNLP parser \cite{not_sure}. 

Gauging the external validity of semantic parsers is difficult. At this point, it seems unclear if the OpenCCG parser is truly capturing the semantics, as classical example sentences often produce different output. However, the field of wide-coverage parsers is relatively immature: Boxer \citep{boxer1} \citep{boxer2}, another popular CCG parser, suffers from the same problem. 

\subsection{Realizer}
Surface realization is essentially the inverse operation to parsing: taking the HLDS and producing a sentence from it. The focus of this paper is presenting a realizer that is more cognitively plausible than previous work, thus allowing psycholinguistic effects to be investigated. 

The realizer works by recursively scoring sentence fragments that were combined with CCG rules. For instance, highly scored words that can be matched together are combined into a fragment, and then they are scored as a fragment. This language model is responsible for deciding which of the possible realizations is most probable.

\subsection{ACT-R}
Generally, surface realizers rely on ngram language models to determine which sentences are the most probable. Contrasting from that, we created a system based on ACT-R, a cognitive architecture \citep{actr}. Architectures such as ACT-R hold that information is retained at different levels of abstraction and in different types of memory. 

\emph{Declarative} memory stores consciously accessible items such as facts. Generally, declarative memory retrieval depends on the recency and frequency of prior exposure, as well as the context. For instance, your wife's anniversary is an element of declarative memory. \emph{Procedural} memory stores more frequently used sequences of steps, which are accessed implicitly, and which are not subject to decay. For instance, how to ride a bike is an element of procedural memory.

It should be fairly clear that lexical items, especially content words, are declarative knowledge: people frequently fail to retrieve uncommon words from memory, resulting in the tip of the tongue phenomenon \cite{needed}. What then is syntactic knowledge?  Some models of language acquisition and production \citep[e.g.,][]{chang2006becoming}  \textbf{also cite lewis' work}   place syntactic information in procedural memory, while others \citep{reitter2011syntacticpriming} place less-frequent items in declarative memory.  The truth may well consist of a combination of both.  Different variants of models will make different predictions, and the text-to-text generation system may be able to compare those models according to fit against a corpus.

%\section{Research Questions and Hypotheses}
%In approaching this problem, we started with two fundamental questions about the methodology of investigating language production on large corpora. The first question is whether classic written language corpora are of actual value in analyzing for the cognitive process of language production. The second is whether or not a traditional realization algorithm would reflect improvements with an increase in cognitive realism. We hypothesized that training on a spoken language corpora as opposed to a written language corpora would show improvements for both models, but possibly a bigger improvement for the ACT-R model. Further, we hypothesized   

%\section{Methodology}
%Our basic idea was a simple performance comparison to demonstrate that this is a valid methodology for investigating cognitively realistic language models. As our control, we used a simple ngram model.  We compared this in performance to a model that took ACT-R activation into account after being trained on two corpora. 

%\subsection{Corpus and Models}
%In order to investigate the claim that we need a spoken language corpus to investigate production realistically, we used the switchboard corpus, part of the Penn Treebank \cite{treebank}. While writing obviously shares some of the same processes as speaking, it does not have the same constraints. In spoken language, the user must plan and produce sentences in real time; when they fail to do so, the signs are often obvious in that they produce some disfluent utterance or pause for an unnatural period of time. While these disfluencies are of interest, they are beyond the scope of this work, so they were removed beforehand. However, a spoken language corpus is still beneficial because in written language, especially professionally written language, the syntactic and lexical choices are often better than can be made in real time. Therefore, it is not necessarily true that the final product of this writing is at all reminiscent of the cognitive processes of producing it, without being able to see the process of revisions.

%In order to show evidence for our first question, whether or not having a spoken language corpus is truly imperative, we used two different training methods. In both methods, the corpus being realized is the switchboard corpus. If we show a statistically significant improvement in the realization by using a spoken language corpus to train over a written language corpus, then we have supported the case that the two are fundamentally different to some extent. 

%The Switchboard corpus itself consists of over six hundred transcribed phone conversations.  These converstions are had by a variety of people and are on a variety of topics. Therefore, in our test model, we trained on n-1 of these conversations and tried to realize the final conversation. Due to the variety in speakers and topics, we believe any effect will be due to the nature of the corpus, rather than due to the model being trained on material that is somehow essentially related. For our control, we used the Wall Street Journal corpus, which is also part of the Penn Treebank.

%Both models were trained using the SRILM toolkit \cite{srilm} which produces a language model in the ARPA format. Both models were trained on 1-4grams and smoothed to better deal with sparse ngrams. 

%\subsection{ACT-R Model}
%In order to ask our second research question, we wanted to show an improvement over the naive method of using the realizer with purely an ngram model. Using this method, we made no novel changes, and simply ran the realizer with a model we trained in order to compare it very directly to our test model.  

%In the second case, we replaced the scoring function that the realizer used to instead use ACT-R base activation. Recall that the principle of ACT-R base activation is in activating chunks in declarative memory that then decay over time. To put this in another way, after you hear or use a word or phrase, it becomes salient in your mind. More presentations more recently means more salience.  ACT-R theory has been used extensively both in language and other fields. In language specifically, it's been used to study syntactic priming \cite{ccg-priming}, learning \cite{learning}, and sentence processing \cite{ungrammatical}, among many more. The most commonly cited formula defines the probability of retrieval in log-odds of a chunk $i$ that has been presented $n$ times $t_j$ seconds ago, with decay rate $d$ and prior probability $\beta_{i}$.

\[B_i = \log {\left[ \sum_{j = 0}^{n} t_{j}^{-d} \right]} + \beta_{i}\]

%The chunks in our model consist of ngrams, instead of just words. While we aren't sure of prior work using this paradigm, we do not consider n-grams for $n = 4$ to be much of a theoretical stretch from $n = 1$. Essentially, this just suggests phrases like "The very quick fox" or "ran away quickly" are also activated, instead of just the words themselves. From a lexical or syntactic perspective, this doesn't necessarily make sense. This view would suggest that ACT-R base activation, which takes solely in declarative memory, would only activate individual words. Then, having retrieved individual words, the syntactic rules, also known as the grammar, would be used to combine them into a complete thought. While we do not argue this perspective directly, we make two basic counter-arguments.

%First, while every ngram is unlikely to be stored in declarative memory for a long time, it's also unlikely that ngrams are never stored in declarative memory. A fairly clearly example of this is the case of referents. A reference to something like "The President of the United States" is unlikely to be created by the combination of 1-grams and syntactic rules, as opposed to retrieved directly from memory. Further, as these ngrams are activated, if they do not consist of a referent (or another single concept), then they are likely to be repeated within the relevant time frame in ACT-R, thus, they'll experience very minimal activation over the baseline. 

%However, for ngrams that cut across syntactic structure, such as "The President of the United [States gave a] speech today", it is both improbable and inelegant to treat the ngram as if it was a declarative chunk, as it would likely bear traces of combinatory rules. However, for now we consider it to be a good enough approximation, especially since our model does not consider syntax in any other way. Furthermore, this is an issue we plan to address in future work, by limiting chunk creation to not cross two phrases.

%The standard value for the decay rate, $d$, is generally $0.5$; however, we used $0.16$ for $d$, which was a result found by Vasishth and Lewis as conforming to sentence processing \cite{sentence} data. This seems to be the closest to an empirically found result for the decay parameter in online language use, so we applied it to our model. Other issues that had to be resolved were the passing of time during the conversation (corresponding to the decay), and the prior exposure to any ngram. We assumed that no ngram that came up in conversation would be novel, and that their prior exposure was to that ngram was related to the ngram frequency in the corpus. Calculating both of these figures required using the oft-cited figure that people speak about 120 words per minute. To calculate the expected prior exposure, because of the exponential decay, we assumed that having a rough estimate for the last time someone was exposed to an ngram would be good enough, because it would dominate previous exposures (and clearly shouldn't be off by more than a factor of about 2). This was basically calculated using the speaking rate, the ngram frequency, and a rough estimate of the percentage of time people spend in conversation: 20\%. While this prior exposure estimate is very approximate, it is not unreasonable and a better approximation should only yield better results.

%For the prior, $\beta_{i}$, we used the ngram frequency in the training corpus. As discussed, a realizer on arbitrary data cannot rely on words and combination mechanisms alone, because the grammars are typically not constrained enough to produce much more than noise. It's of course been demonstrated many times that it's possible to create nonsense sentences with sensible words and sensible combinatory mechanisms. While ideally, a perfect set of semantics would allow us to ignore ngrams, the model would still need a way to choose between two semantically-equivalent sentences. While there has certainly been some speculation on this topic, there is very little specific information presently on how people carry that out. For now, we consider the ngram prediction to be a necessary part of the model, and there is decent evidence that listeners make predictions that are reminiscent of ngram models, perhaps by mimicking the production process \cite{prediction}\cite{integrated}. Further, while base activation isn't strictly Bayesian, the notion of updating the base rate of ngram occurrence with the context falls roughly in line with the many who posit humans as following Bayesian rationality.

%While it may seem like abandoning all of ACT-R for a single formula is a weak case for cognitive plausibility. However, Reitter et al made the case for more streamlined models that only require the theoretically driven components of the model \cite{actup}. While there are still many more theories that could be implemented, we see this as a victory for our methodology rather than a drawback of our current work. 

\subsection{Evaluation}
%After the realizations for both models were generated, they were evaluated with ROUGE, or Recall-Oriented Understudy for Gisting Evaluation \cite{rouge}. ROUGE is a metric that was developed to compare automatic summarizations and has also been used in machine translation. Effectively, the machine-generated summarizations and translations are compared against gold-standard translations or summarizations, usually produced by an expert.  We ran ROUGE at the conversation-level: that is, each conversation generated by the realizer was rated for its total precision and recall against the gold standard conversation, which in this case was simply the text of the original conversation. These values are then averaged over all of the conversations and given a confidence interval. 
%As the ROUGE metric relies largely on ngrams and common subsequences, it is unclear that it should be used to evaluate semantic equivalence. After all, two semantically equivalent expressions don't necessarily fare well with regards to ngrams compared to more similar (but ultimately nonsensical) sentences. However, in all cases, the uses of the ROUGE metric are for measuring semantics. A better summary or translation evaluation should also be closer semantically, rather than closer in the exact wording. Therefore, unless a better metric becomes available, we see ROUGE as plenty applicable to this domain. 

\section{Results}
%To test our first research question, we compared the ROUGE score of our model trained on the Wall Street Journal corpus with the ROUGE score of our model trained on the SWBD corpus. 

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|cc|}
\hline
 &ACT-R&Standard \\
\hline
Recall &  &  \\
\hline
Precision & & \\
\hline
F-Measure && \\
\hline
\end{tabular}
\end{center}
\caption{\label{swbd-results} Comparison of ACT-R model vs. Ngram model on SWBD. }
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|cc|}
\hline
 &ACT-R&Standard \\
\hline
Recall &  &  \\
\hline
Precision & & \\
\hline
F-Measure && \\
\hline
\end{tabular}
\end{center}
\caption{\label{swbd-results} Comparison of ACT-R model vs. Ngram model on WSJ. }
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|cc|}
\hline
 &ACT-R&Standard \\
\hline
Recall &  &  \\
\hline
Precision & & \\
\hline
F-Measure && \\
\hline
\end{tabular}
\end{center}
\caption{\label{swbd-results} Comparison of SWBD corpus vs. WSJ with ACT-R model}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|cc|}
\hline
 &ACT-R&Standard \\
\hline
Recall &  &  \\
\hline
Precision & & \\
\hline
F-Measure && \\
\hline
\end{tabular}
\end{center}
\caption{\label{swbd-results} Comparison of SWBD corpus vs. WSJ with Ngram model}
\end{table}

\section{Discussion}
%There are several important factors to consider when looking at the results for the ACT-R model. The most important point is that it underwent extremely minimal tuning, using values that seemed reasonable rather than values that optimized the results. The prior probability for the model ($\beta_i$) certainly seems reasonable, but the point at which the model's assumption breaks down (ngrams as declarative chunks) could potentially have impacts on the model's ability to improve upon the baseline. Therefore, the size of the ngram model, the decay rate of chunks, and the prior exposure to ngrams are all things that could easily be experimented upon further without any fundamental changes to the model-theoretic predictions. 

%Nonetheless, despite this drawback, we were able to find mildly compelling evidence that the ACT-R model was able to outperform the standard ngram model. We consider this as a victory for our methodology, especially given the relative simplicity of the changes we made. We find this to be compelling evidence that our methodology can be used as a more general cognitive modelling platform. While the ACT-R system is certainly useful, the interesting part of any cognitive model is the theory. If our model implements the ACT-R theory (or a subset of it) and achieves consistent results, then the work is no different than if it was actually implemented in the ACT-R system. While currently our model only implements a very small subset of ACT-R theory, this can be expounded upon in further papers.

%Note that we don't claim an improvement to the state of the art of text to text generation, nor do we think it is important to for our current goals. Rather than see our results as an extension of the field of text to text generation, we see it as a unification of text to text generation with cognitive modelling. If an algorithm is able to produce from a set of semantics the same text more quickly or more accurately than a human, this is not of interest to a psycholinguist. However, if the model is grounded in the mechanisms and constraints of the human language production system, it can be used to shed light on the process by comparing to a control. 

%Our work here compared the usage of context to a control, and  found an improvement. Our goal certainly wasn't to validate the theory by implementing it in the system, but to validate the system by showing that it reflects improvements with the application of this theory, in much the same way that humans do. Humans show significant improvement in the task of language when the context is clear, as has been found multiple times (citation needed). By demonstrating this improvement, we have shown initial support for our model as a framework for investigating such phenomenon. While it is unclear if the realization algorithm itself was originally intended to be used this way, its combinatory mechanism being based in a theoretically-driven grammar like CCG allows it to be co-opted for this purpose.

%Our results find moderately compelling evidence that spoken language and written language corpora should not be interchanged. This should not be surprising given the procedural differences discussed earlier; however, in many language studies, written language data are used to investigate psycholinguistic phenemonon. Until such data sets are better understood (for instance, Wall Street Journal may be less like spoken language than twitter, chat rooms, or even forums), this fact must be acknowledged. While we're certainly not suggesting the phenomenon contained in these other types of data are not interesting or are completely disjoint from spoken language, it seems probable that the process is different. 

%However, as we found such results for both the normal ngram and the actr model, it's difficult to conclude if these corpora are different in a way that would express itself cognitively. A further study on a better tuned model may be able to find a larger difference in the ACT-R model, as we expected. Finding this larger difference would show preliminary evidence that the two types of text are different in a way that's related to cognitive processes. On the other hand, it's possible that just the topics of conversation and the mannerisms used in spoken language are different than written language. If this is the case,  then the usage of these corpora to investigate cognitive phenomenon is in no way threatened, so long as the usage is consistent. A more complete model should be able to begin to answer this question.
  
\section{Further Work}
%Our current model is presented as a baseline that could easily be expanded upon to investigate a wide range of linguistic phenomenon in corpora. Beyond obvious tuning, there are some very obvious improvements we can make going forward. For instance, implementing the effects due to  priming: participants in a conversation tend to converge to a common vocabulary a way that's more nuanced than simple base activation. Interestingly, the model could be extended in ways that are expected to make its coverage worse. For instance, there is the well known effect of various kinds of interference, such as attempting to read the word green written in red ink. Successfully modelling these types of phenomenon on corpus data would still be a step forward, even if the model's actual coverage worsens.  Beyond implementing well-understood phenomenon, this platform could be used to investigate controversial ones. In many complicated areas, such as the degree that production planning is incremental, there is wide-spread disagreement. To our knowledge, no one has investigated these with cognitive modelling, let alone cognitive modelling on big data.



\section{Conclusion}
In this paper, we present a new methodology for investigating language production. This work makes heavy use of the algorithms and methodology from White's language realization work. However, instead of looking for an improvement in the state of the art to Text-toText Realization, we modified the system to be used as a platform for investigating language at the cognitive level. To test the viability of this platform, we replaced the algorithm's scoring function with ACT-R base activation. Additionally, we examined the viability of using corpora that were not spoken language as part of this system. We found results 

\printbibliography

\end{document}
