\section{Evaluation}
Naturally, while the model produces utterances, those utterances don't always make sense. Quantifying exactly what it is to make sense is obviously difficult, so we went with a simpler metric: what is probable. We scored each sentence using an ngram model using the SRILM toolkit trained on the BNC corpus \citep{srilm}. While we could have compared the utterance to the original sentence, since we are mostly bypassing the semantic representation component, we are more concerned with the model producing any good utterances, rather than the ones with the intended meaning. Obviously, the ngram score produced for any given sentence will vary depending on the content of that sentence; however, the distribution of the scores can still tell us how close each variant of the model is to actual human language. Naturally, ngram scores of human language do not follow a normal distribution, so simple statistics are not particularly useful. Instead, we turn to the wilcox test, which determines whether two sets of samples are from the same distribution. If certain conditions are indistinguishable from human language, while others are not, that would suggest something about that condition is more similar to human language. 

There are also more qualitative analyses we can do. An important feature of most cognitive processes is resource minimization. In other words, processes are trying to minimize memory usage, cognitive load, and realization time. Cognitive load is fairly difficult to measure using our process: it would depend, in part, when semantic ideas enter working memory, which is out of the scope of our current model's process. Realization time, depending largely on the length of the utterance, is also difficult to measure. However, memory usage just refers to the number of items in memory as the realization process proceeds. We defined every syntax node in memory as an item, and if two syntax nodes are joined, then so is the item in memory. While using working memory as an evaluation in of itself wouldn't show anything, if it corresponds to the other data, then it provides additional support for the methodology.

Also of interest is the correspondence between position in the sentence and the actual syntactic analysis of the result. Sentences are generally considered right-branching if they start with syntax nodes on the left, progressively adding syntax nodes to the right. Left-branching is the opposite: syntax nodes on the right are appended on the left. Obviously, sentences are not (in general) entirely right-branching or left-branching, but instead, a numerical value can be assigned to measure this. Our methodology was done by considering all subtrees: every subtree had some amount of left and right nodes, which are then tallied. A tree's right-branching score is the total number of nodes to the right divided by the total number of nodes to the left. Thus, a higher score means that the tree is more right-branching. We should expect this right-branching factor to correspond to the utility initializations, if the realizer is producing the same sentences as the original sentences. 

On the other hand, the utility initializations might not be teaching the realizer how to produce those same sentences, but something more fundamental about word order. It is possible, for instance, that certain phrases or syntactic category combinations are more likely to appear because of utility. To test this, we can actually compare the produced sentences to the target sentences, using a metric like edit distance. There are a few ways to measure edit distance, which would have different implications. For instance, if a slot-based approach is used, then phrases that are out of place would get no benefit. However, if a metric like Levenshtein distance is used, then it would take fewer moves to restore the original sentence if there are shared phrases. The correlations between these metrics and the others could inform us about the correct interpretation of our results.