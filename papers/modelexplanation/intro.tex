\section{Introduction}
 Studying language production in an effective way thus far has been hard. Psycholinguistics generally relies on carefully controlled experiments; however, studying production in this way is difficult because participants are presented with a large degree of choice. For example, if you ask a subject to describe a picture without controlling what he or she will say, there are a (theoretically) infinite number of utterances he or she could produce. Due to the difficulty in collecting large amounts of data in such a setting, comparing a massive number of experimental conditions is not really feasible. \\
 \indent For instance, consider an experimenter who wishes to contrast the choice of a Double Object construction (The boy threw his dog the ball) with a Prepositional Object construction (The boy threw the ball to his dog). If the participant is simply describing a picture, there is no guarantee he or she will even reference the dog! The primary strategy for countering this problem has been to provide the participant with the syntactic structure of the sentence \citep{incremental} or with the first few words of the utterance \citep{chinese}. In the latter of these studies, the researchers still had to include an "Other" category for sentences that did not fit in any of the desired conditions. \\
 \indent While these controls have been useful, they could have the side effect of muddling certain parts of the process: such as planning. In order to produce a sentence, it must first be planned. Investigating the planning process is difficult: many paradigms frequently used to investigate sentence comprehension, such as eye tracking or the visual world, seem much more difficult to apply. Despite the difficulty in designing experiments to measure planning, it is clearly of interest to researchers, as can be evidenced by ongoing debates. One such debate is to what degree is the process \emph{incremental}. \\
 \indent For instance, with sentences that contain center-embedded clauses, is the end planned before or after the embedded clause? Consider the sentence "The dog that was chasing the cat that seemed to have rabies fell over." An incremental derivation would rely on \emph{surface order}: the actual order the words appear in the sentence. In other words, the speaker first plans "the dog", then "that was chasing the cat", then "that seemed to have rabies" and lastly "fell over." A less incremental derivation might be to plan "fell over" immediately after "the dog", and then to add the clauses later. \\
 \indent Some researchers seem to take it as a given that language processes are radically incremental (especially with regards to comprehension) \citep{tag1}\citep{radical}. However, the evidence (especially for production) is not so clear \citep{incremental}. Due to the seeming difficulty in solving this debates such as these with more experimentation, we suggest turning to computational cognitive modeling.\\
 \indent Computational cognitive models of language production are not novel in of themselves \citep{model}. However, to some extent, they suffer from the same problem as experimentation. The breadth of linguistic diversity falls short of the sentences that any experimenters can plan for. To solve these problems, we need computational cognitive models with wide coverage. We believe our model is a step in this direction.
 
\section{Goals}
Our goal is to achieve this using the tight constraints of the ACT-R system \citep{actr}. We further attempt to stick within the constraints of plausibility by not relying on strategies such as resampling from previously heard input, due to its inability to produce novel productions. Instead, we define a range of operations that are defined in the Combinatory Categorial Grammar (CCG) \citep{ccg}, a minimally context sensitive grammar that is believed to be capable of all of the syntactic operations of human language \citep{convergence}. However, while our current system uses the operations of CCG, our theoretical basis of combinatory operations does not explicitly rely on CCG. Indeed, our system could in the future compare two combinatory systems for plausibility. Our longterm goals take nothing for granted: any operation, method, or structure that is cognitively plausible, fits the data, and produces realistic output is a possibility. 

In the short term, however, we make several assumptions about the operations and structure of language production. The model has two basic modes to accomplish this. One can be thought of as \textit{naturalistic} mode. In naturalistic mode, the model attempts to combine whatever words it wants, given its current settings, rules, chunks, and goals. In this mode, it is natural for the model to make some mistakes, but it can still be compared with itself for \textit{plausibility}. The second mode is \textit{tracing} mode. In tracing mode, the model will be corrected if it attempts to produce something incorrect. Then, the number of mistakes made are tallied over time. While the number of mistakes will still be high, they should decrease over time. In this way, we are comparing two models for \textit{learnability}. 

\section{Evaluation}
The model receives as input a set of words that made up a sentence in the Switchboard corpus. The model will then, using its knowledge about these words and possible combinatory operations, attempt to produce a sentence or sentence fragment. Each mode will have a different evaluation metric, and the questions we hope to ask are also slightly different. In tracing mode, the goal of the model is to reduce the number of mistakes by more than the control model. We expect model that learns more quickly to be a better model of language production than one which has more trouble learning. Thus, we will compare error differences after several benchmarks between the two models to determine which is more learnable. In naturalistic mode, the goal is to be more plausible. Thus, we will compute the average ngram scores using the SRILM toolkit \citep{srilm} for the utterances of each model. As the models are generating from the same bag of words, there should be no disadvantage to choosing uncommon but naturalistic phrases, especially over a large dataset. However, there is a possibility longer utterances are naturally less probable, so some adjustment may be necessary. Further, computing ngram scores for ngrams larger than five words is fairly unrealistic with current technology. It is possible that some related metric, such as the average of all 3-gram scores, could solve both of these problems. 

\section{ACT-R}
ACT-R has a few basic units of organization. The primary units for declarative memory are \textit{chunks}. A chunk is a fairly simple concept that basically refers to one thing that can be retrieved or held in working memory. Chunks can be arbitrarily simple or complicated by way of \textit{slots}. A slot is a simple data type that corresponds to another chunk. The most primitive chunks thus have a name, but no slots. 

Buffers are a unit of cognition that can hold exactly one chunk. While ACT-R can, in theory, have an arbitrary number of buffers (including those such as the visual buffer or the auditory buffer), we make use of only the simplest buffers: the retrieval buffer and the goal buffer. The retrieval buffer can be thought of as the state of memory retrieval. It can be empty or it can be retrieving something or it can contain something it just retrieved. The goal buffer, on the other hand, can be thought of as working memory. It also can only contain one chunk, but it contains the state of the problem that is being solved. 

Production rules are the allowable manipulations of the buffers, and the conditions upon which those manipulations should be performed. For instance, a model could have a production rule that says to retrieve something when the goal buffer is at some state. It might, for instance, call for looking up the syntactic type of the word that's currently in a given slot in the goal buffer. These production rules, in combination with chunks, form a cognitive model of some task when by meeting these conditions iteratively, they can successfully transform the initial state of the problem into its goal state. 

ACT-R additionally has a couple of other mechanisms. One is a form of simple utility learning, where after a rule fires, it is rewarded, penalized, or neither. We will use this type of learning with the tracing model. Another is activation, which affects which matching chunk is retrieved. This activation can be used to explain many linguistic phenomenon, such as priming \citep{priming} \citep{model}.  

In the following, we'll present the full set of chunks and production rules that make up our model. 