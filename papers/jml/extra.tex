\subsection{Goals}
Our goal is to achieve this using the tight constraints of the ACT-R system \citep{actr}. We further attempt to stick within the constraints of plausibility by not relying on strategies such as resampling from previously heard input, due to its inability to produce novel productions. Instead, we define a range of operations that are defined in the Combinatory Categorial Grammar (CCG) \citep{ccg}, a minimally context sensitive grammar that is believed to be capable of all of the syntactic operations of human language \citep{convergence}. However, while our current system uses the operations of CCG, our theoretical basis of combinatory operations does not explicitly rely on CCG. Indeed, our system could in the future compare two combinatory systems for plausibility. Our longterm goals take nothing for granted: any operation, method, or structure that is cognitively plausible, fits the data, and produces realistic output is a possibility. 

In the short term, however, we make several assumptions about the operations and structure of language production. The model attempts to combine whatever words it wants, given its current settings, rules, chunks, and goals. It is natural for the model to make some mistakes, but it can still be compared with itself for \textit{plausibility}.

\subsection{Evaluation}
The model receives as input a set of words that made up a sentence in the Switchboard corpus. The model will then, using its knowledge about these words and possible combinatory operations, attempt to produce a sentence or sentence fragment. The goal is to be more plausible, though this is obviously somewhat difficult and controversial to measure. Our method was to average the bigram scores produced by the SRILM toolkit \citep{srilm} for a given sentence. We used the same method for evaluation as used by \citet{chart}. Then, we computed standard statistical metrics on the distribution of these scores: namely the mean, median, and standard error.

As the models are generating from the same bag of words, there should be no serious disadvantage to choosing uncommon but naturalistic phrases, especially over a large dataset. 