#!/usr/bin/python

################## NOTE NOTE NOTE ##################
#
# This file (ccg2xml.py) was automatically generated from ccg.ply.
# Generated by convert-ply.py at Mon Jan 19 18:32:31 2015.
#
# DO NOT MODIFY THIS FILE DIRECTLY.  YOUR CHANGES WILL BE LOST.
# Instead, modify the file `ccg.ply' that generated this file, and then
# rerun `convert-ply.py -o ccg2xml.py ccg.ply'.
#
################## NOTE NOTE NOTE ##################


# Author: Ben Wing <ben@666.com>
# Date: November 2006

#############################################################################
#                                                                           #
#                                 ccg.ply                                   #
#                                                                           #
#   Convert a .ccg file, specifying a CCG grammar, into files lexicon.xml,  #
#   rules.xml, morph.xml, types.xml and grammar.xml.  You can't actually    #
#   run this file itself; you have to use convert-ply.py to convert it      #
#   into a Python file (ccg2xml), which you then run to generate the XML    #
#   files from the .ccg input.  For a description of the format of this     #
#   file, see the comments in convert-ply.py.                               #
#                                                                           #
#############################################################################


import sys
import re
import optparse
import copy
import os
import cStringIO

# Local imports
import lex, yacc

from Tkinter import *
from tkMessageBox import *
import tkFont

## Get options

usage = """%prog [OPTIONS] FILE ...

Generate appropriate XML files for input to OpenCCG.
"""

parser = optparse.OptionParser(usage=usage)
parser.add_option("-o", "--omit-output",
                  help="""Omit the specified files from the output.
Value should be a list separated by commas or spaces.  The allowed values are grammar, morph, lexicon, rules, types, and testbed.  If you put a + sign before the list, it means output *only* the specified files."""
                  )
parser.add_option("-p", "--prefix",
                  help="""Optional prefix to attach to each of the generated files, so that output from different files can occur in the same directory.  Defaults to the base name of the input file, minus any extension, plus a hyphen.  If you want such a hyphen or similar char, add it yourself.""",
                  metavar="DIR"
                  )
parser.add_option("-d", "--dir",
                  help="""Directory to store files in (defaults to current directory).""",
                  metavar="DIR"
                  )
parser.add_option("-q", "--quiet",
                  action="store_true",
                  help="Don't output explanatory messages, but only warnings and errors.")
parser.add_option("-t", "--transformed-input",
                  action="store_true",
                  help="Output transformed input after macro substitutions have been applied.")
parser.add_option("-y", "--yacc-debug",
                  action="store_true",
                  help="Show more output about the YACC parser generation.  Also probably generate some extra files, e.g. parser.out, containing info about the generated parser.")
parser.add_option("-m", "--macro-debug",
                  action="store_true",
                  help="Dump macro definitions at end of file.")
parser.add_option("--super-macro-debug",
                  action="store_true",
                  help="Show copious output about macro expansions.")

def parse_arguments(argv):
    global options, global_args
    (options, global_args) = parser.parse_args(argv)

    # Global variables used for debugging; we may move them into the
    # global-state variable
    global lex_debug
    global xml_debug
    global yacc_debug
    global macro_debug
    global super_macro_debug

    lex_debug = 0
    xml_debug = 0
    yacc_debug = options.yacc_debug
    macro_debug = options.macro_debug
    super_macro_debug = options.super_macro_debug

########################################################################
#                           Utility functions                          #
########################################################################


# CONVENTIONS:
#
# --------- XML ----------
#
# Thankfully, the structure of XML is extremely simple.  We represent
# a single XML statement of the form
#
# <biteme foo="1" blorp="baz">
#   <bitemetoo ...>
#     ...
#   gurgle
# </biteme>
#
# as a list
#
# ['biteme', [('foo', '1'), ('blorp', 'baz')],
#    ['bitemetoo', ...],
#    'gurgle'
# ]
#
# i.e. an XML statement corresponds to a list where the first element
# is the statement name, the second element lists any properties, and
# the remaining elements list items inside the statement.
#
# ----------- Property lists -------------
#
# The second element of an XML statement in list form is a "property list",
# a list of two-element tuples (property and value).  Some functions below
# (e.g. `getprop', `putprop') manipulate property lists.
#
# FIXME: Just use a hash table.
#
# ---------- Abstract syntax trees -----------
#
# We use classes to represent statements and blocks.  Below this level, it's
# simpler to just use the XML that we ultimately have to generate anyway.
# The conventions for using XML are either to use property lists or lists of
# XML statements in the list form outlined above.


#############################
#        Handling XML       #
#############################

def xml_sub(crap):
    if type(crap) is not str:
        crap = str(crap)
    crap = crap.replace('<', '&lt;')
    crap = crap.replace('>', '&gt;')
    return crap

def print_xml_1(file, xml, indent=0):
    if xml_debug > 1:
        errout("%sPrinting: %s\n" % (' ' * indent, str(xml)))
    if type(xml) is not list:
        file.write('%s%s\n' % (' ' * indent, xml_sub(xml)))
    else:
        check_arg_type("XML statement", xml[0], str)
        file.write(' ' * indent)
        file.write('<%s' % xml_sub(xml[0]))
        for x in xml[1]:
            check_arg_type("XML statement", x, tuple)
            if len(x) != 2:
                raise TypeError("Bad tuple pair: " + str(x))
            file.write(' %s="%s"' % (xml_sub(x[0]), xml_sub(x[1])))
        subargs = xml[2:]
        if not subargs:
            file.write('/>\n')
        else:
            file.write('>\n')
            for x in subargs:
                print_xml_1(file, x, indent + 2)
            file.write(' ' * indent)
            file.write('</%s>\n' % xml_sub(xml[0]))

# Pretty-print a section of XML, in the format above, to FILE.
# Start at indent INDENT.

def print_xml(file, xml):
    if xml_debug == 1:
        errout("Printing: %s\n" % str(xml))
    print_xml_1(file, xml)

# Return True if PROP is seen as a property in PROPLIST, a list of tuples
# of (prop, value)
def property_specified(prop, proplist):
    return not not ['foo' for (x,y) in proplist if x == prop]

# Return value of property PROP in PROPLIST; signal an error if not found.
def getprop(prop, proplist):
    for (x,y) in proplist:
        if x == prop:
            return y
    raise ValueError("Property %s not found in %s" % (prop, proplist))

# Return value of property PROP in PROPLIST, or DEFAULT.
def getoptprop(prop, proplist, default=None):
    for (x,y) in proplist:
        if x == prop:
            return y
    return default

# Replace value of property PROP with VALUE in PROPLIST.
def putprop(prop, value, proplist):
    for i in xrange(len(proplist)):
        if proplist[i][0] == prop:
            proplist[i] = (prop, value)
            return
    else:
        proplist += [(prop, value)]
    

# Replace property named PROP with NEW in PROPLIST.  Often this is called with
# with PROP equal to None; the None occurs when a PROP=VALUE clause is expected
# but a bare value is supplied.  The context will supply a particular default
# property (e.g. 'name') to be used when the property name is omitted, but the
# generic code to handle property-value clauses doesn't know what this is.
# The surrounding code calls property_name_replace() to fill in the proper name.

def property_name_replace(prop, new, proplist):
    for i in xrange(len(proplist)):
        if proplist[i][0] == prop:
            proplist[i] = (new, proplist[i][1])

#############################
#      Error-handling       #
#############################

def init_errors(errors_to_string):
    # Count of number of errors seen so far.
    global error_count
    error_count = 0

    global write_errors_to_string
    write_errors_to_string = errors_to_string

    global stdout_file, stderr_file
    if errors_to_string:
        stdout_file = cStringIO.StringIO()
        stderr_file = cStringIO.StringIO()
    else:
        stdout_file = sys.stdout
        stderr_file = sys.stderr

    global message_log
    message_log = []

def save_errors(cur):
    cur.error_count = error_count
    cur.write_errors_to_string = write_errors_to_string
    cur.stdout_file = stdout_file
    cur.stderr_file = stderr_file

class InternalError(StandardError):
    pass

def argformat(format, arg):
    if type(format) is str:
        return format % arg
    else:
        return str(format)

# Throw an error, like fprintf(stderr, ...)
def synerr(format, *arg):
    raise SyntaxError(argformat(format, arg))

# Output to stderr, maybe.  But output to stdout if our input is being
# output at the same time, so the two will stay in sync.
def maybe_errout(str):
    # Force display of error
    # FIXME: Maybe we could dump all errors into a single 
    # window display and show the messages together
    #showerror('Message', str)

    if options.transformed_input:
        stdout_file.write(str)
    else:
        stderr_file.write(str)

def error_or_warning(title, lineno, format, *arg):
    formatted_arg = argformat(format, arg)
    if lineno:
        maybe_errout("%s, line %s: %s\n" % (title, lineno,
                                            formatted_arg))
    else:
        maybe_errout("%s: %s\n" % (title, formatted_arg))
    # Add the message as a tuple, for easy recall in the editor
    # Note: lineno being put in irrespective of the fact
    # of whether it exists or not
    # FIXME!! The purpose of errors_to_string and message_log duplicate
    # each other somewhat.  Clean up.
    global message_log
    message_log += [(title, lineno, formatted_arg)]

# Write formatted arguments to stderr, with Error: printed.
def error(lineno, format, *arg):
    global error_count
    error_count += 1
    error_or_warning('Error', lineno, format, *arg)

# Write formatted arguments to stderr, with Warning: printed.
def warning(lineno, format, *arg):
    global warning_count
    warning_count += 1
    error_or_warning('Warning', lineno, format, *arg)

# Write formatted arguments to stdout.
def outout(format, *arg):
    stdout_file.write(argformat(format, arg))

# Write formatted arguments to stderr.
def errout(format, *arg):
    stderr_file.write(argformat(format, arg))

# Debugging output: Always to sys.stderr.
def debug(format, *arg):
    sys.stderr.write(argformat(format, arg))

def check_arg_type(errtype, arg, ty):
    if type(arg) is not ty:
        raise TypeError("%s: Type is not %s: %s" % (errtype, ty, arg))

#############################
#   Abstract Syntax Trees   #
#############################

# Classes beginning with CS (= CCG Syntax) are used for constructing the
# abstract syntax tree corresponding to a CCG source file. (An abstract
# syntax tree, or AST, is a hierarchical representation of the syntax of a
# piece of source code text, in this case a CCG-format file.) The source
# file is made up out of blocks, each of which begins with an identifier
# and is followed by one or more statements.

# A CSNode corresponds to any unified section of source code -- a single
# block or statement, a particular part of a statement (e.g. an
# attribute-value list or a single attribute-value clause), or even the
# whole file.  The basic restriction is that it must correspond to a single
# YACC production; hence it logically belongs in a unit and is the maximum
# extent of text that belongs in the unit or possibly statement in a single
# block.  It has some corresponding source text with starting and ending
# line numbers, a function to generate the XML, and a function to draw the
# node.  If the node is large enough to represent at least one XML
# statement, it should be a list of XML statements in the XML-statement
# form described above (a list [TAG, PROPLIST, CHILD ...]); otherwise, the
# format is undefined, but most likely will be a property list.  The CSNode
# is initialized from the YaccProduction object (stored in variable `p',
# usually, but accessed as $@) associated with a particular production,
# which supplies the extent of source code associated with the production.

class CSNode(object):
    def __init__(self, prod):
        self.prod = prod
    def xml(self):
        # In many cases, it's easiest just to build up the XML at creation time
        # and store it, rather than constructing it dynamically.  Note that we
        # intentionally don't initialize self.static_xml, so we get an error
        # if it's not set.
        return self.static_xml
    # draw(self, parent, cfile, vars): Draw the node by returning a new widget
    # containing the drawn representation: Should be defined if node is
    # drawable.  It should return a widget that is a child of PARENT,
    # also a widget.  It is up to the caller to call pack() or grid()
    # so that the widget's geometry will be set; but the draw() function
    # should appropriately configure any child widgets that it creates.
    # VARS is an object containing Tkinter variables that may control the
    # way that the node is drawn.

# A CSStatement corresponds to a single statement in a single block.  Note
# that, in the interests of simplicity, we don't currently create objects
# for pieces of CCG code that are smaller than a statement; instead, we
# just use the XML representation.  We usually follow the convention that
# if we have to make changes to the XML that make it not be in a one-to-one
# correspondence with the original code, we do this at the level of the
# statement or block.

class CSStatement(CSNode):
    def __init__(self, prod):
        super(CSStatement, self).__init__(prod)

# A CSBlock is a single block.

class CSBlock(CSNode):
    def __init__(self, prod):
        super(CSBlock, self).__init__(prod)

#############################
#             Misc          #
#############################

# Is it identifier material?  Input should be a character.
def isalnumund(str):
    return str.isalnum() or str in '_+-'

# Prior to Python 2.4, no sorted()
def my_sorted(lyst):
    lystcopy = list(lyst)
    lystcopy.sort()
    return lystcopy

########################################################################
#                               Tokenizing                             #
########################################################################

# The following IDs have a special meaning to the OpenCCG tokenizer if a
# token has the form [*ID*].
#magic_names = ('AMT', 'DATE', 'DUR', 'NUM', 'TIME')

# It seems that the tokenizer does not require an [*ID*] token to be a known
# magic thing (i.e. [*FOO*] is a legal surface form), so line 402 is commented
# out.
# If someone decides that [*ID*] tokens should be restricted to the ones
# listed above, uncomment lines 397 and 503.

# Directives -- These are particular words that are specially handled in
# an appropriate position and hence need to be tokens for use in the
# parser.  However, they can also be part of a generic "word" -- in
# other words, we have no "reserved words".

directives = (
    'FAMILY', 'ENTRY', 'MEMBER', 'FEATURE', 'PROP', 'RULE',
    'NO', 'APP', 'COMP', 'XCOMP', 'SUB', 'XSUB', 'TYPERAISE', 'TYPECHANGE',
    'DEF', 'WORD', 'TESTBED', 'RELATION_SORTING'
    )

# Additional tokens that can form part of a word.  A bare 'x' can form
# part of a word as well, except for in a few circumstances.
basic_word_no_x_tokens = ('ID', 'QUOTEDID') + directives
word_no_x_tokens = ('NUMBER',) + basic_word_no_x_tokens
word_no_number_tokens = ('X',) + basic_word_no_x_tokens
word_tokens = ('NUMBER',) + word_no_number_tokens + ('MAGIC_ID',)
bracket_tokens = ('LPAREN', 'RPAREN', 'LBRACKET', 'RBRACKET',
                  'LBRACE', 'RBRACE')
other_tokens = (
    # String tokens
    'SLASH', 'BACKSLASH',
    'LESS', 'GREATER',
    'CARET', 'STAR', 'DOT', 'AT', 'EQUALS', 'GOESTO', 'PIPE',
    'COMMA', 'SEMI', 'DOLLAR', 'COLON', 'BANG', 'TILDE',
    # Handled through t_ID
    'PLUS', 'MINUS', 'PLUSMINUS',
    # Only in a def()
    'NEWLINE',
    'BOGUS_VALUE' # Kludge kludge kludge, fuck me harder
    )

tokens = word_tokens + bracket_tokens + other_tokens

t_LPAREN     = r'\('
t_RPAREN     = r'\)'
t_LBRACKET   = r'\['
t_RBRACKET   = r'\]'
t_LBRACE     = r'\{'
t_RBRACE     = r'\}'

t_SLASH      = r'/'
t_BACKSLASH  = r'\\'
t_LESS       = r'<'
t_GREATER    = r'>'
t_CARET      = r'\^'
t_STAR       = r'\*'
t_DOT        = r'\.'
t_AT         = r'@'
t_EQUALS     = r'='
t_GOESTO     = r'=>'
t_PIPE       = r'\|'
t_COMMA      = r','
t_SEMI       = r';'
t_DOLLAR     = r'\$'
t_COLON      = r':'
t_BANG       = r'!'
t_TILDE      = r'~'

# Identifiers and directives

directives_map = { }
for r in directives:
    directives_map[r.lower().replace('_', '-')] = r
directives_map['x'] = 'X';
# We handle +, -, and +- here because + and - can, in general, form part
# of a token.
directives_map['+'] = 'PLUS';
directives_map['-'] = 'MINUS';
directives_map['+-'] = 'PLUSMINUS';

def t_ID(t):
    r'''(([\-+%a-zA-Z_0-9]|[^\000-\177])+|"[^"\n]+"|\'[^'\n]+')'''
    # convert to directive, maybe
    if re.match(r'^\d+$', t.value):
        t.type = 'NUMBER'
        try:
            t.value = int(t.value)
        except ValueError:
            error(t.lineno, "Integer value too large: %s", t.value)
            t.value = 0
    elif t.value in directives_map:
        t.type = directives_map[t.value]
    else:
        t.type = 'ID'
        # remove quotes if they're there
        if t.value[0] == '"' or t.value[0] == "'":
            t.type = 'QUOTEDID'
            t.value = t.value[1:-1]
    return t

# The distinction from ordinary IDs is currently not really needed, i.e. t_ID
# could to the job, too. However, it leaves open the possibility to handle
# magic IDs differently from ordinary ones.
def t_MAGIC_ID(t):
    r'''(\[\*[^*]+\*\])'''
    t.type = 'MAGIC_ID'
    return t
#t_MAGIC_WORD.func_doc = '(\[\*(' + '|'.join(magic_names) + ')\*\])'

t_ignore = " \t\r"

#bracketmap = {'(': 'LPAREN', ')': 'RPAREN',
#              '[': 'LBRACKET', ']': 'RBRACKET',
#              '{': 'LBRACE', '}': 'RBRACE'}
#
#def t_LBRACKET(t):
#    r'[\[\(\{]'
#    global parendepth
#    parendepth += 1
#    t.type = bracketmap[t.value]
#    return t
#
#def t_RBRACKET(t):
#    r'[\]\)\}]'
#    global parendepth
#    parendepth -= 1
#    t.type = bracketmap[t.value]
#    return t

def t_backslash_newline(t):
    r'\\\r?\n'
    t.lineno += 1
    # If it's not a line continuation, it's just a normal backslash
    if not lexer_track_newlines:
        t.type = 'BACKSLASH'
        return t

def t_newline(t):
    r'\n'
    t.lineno += 1
    if lexer_track_newlines:
        t.type = 'NEWLINE'
        return t

# Comments
def t_comment(t):
    r'\#[^\n]*\n'
    t.lineno += 1

def t_error(t):
    error(t.lineno, "Illegal character '%s'", t.value[0])
    t.skip(1)
    
def init_lexer():
    # This is a signal to us to go into "line mode", where we return a
    # newline as a token and treat backslash at the end of a line as a line
    # continuation device.
    global lexer_track_newlines
    lexer_track_newlines = 0

    # Build the lexer.  This does introspection, on all the t_*() functions.
    global globallexer

    globallexer = lex.lex(debug=lex_debug)

def save_lexer(cur):
    cur.lexer_track_newlines = lexer_track_newlines
    cur.globallexer = globallexer

########################################################################
#                                Parsing                               #
########################################################################

def p_word(p):
    'FILLED IN BELOW'
    p[0] = p[1]
# fill in the documentation (i.e. the cfg rule)
p_word.func_doc = 'word : ' + '\n| '.join(word_tokens)

# hack, to deal with a reduce/reduce conflict
def p_word_except_x(p):
    'FILLED IN BELOW'
    p[0] = p[1]
# fill in the documentation (i.e. the cfg rule)
p_word_except_x.func_doc = 'word_except_x : ' + '\n| '.join(word_no_x_tokens)

def p_word_no_numbers(p):
    'FILLED IN BELOW'
    p[0] = p[1]
# fill in the documentation (i.e. the cfg rule)
p_word_no_numbers.func_doc = (
    'word_no_numbers : ' + '\n| '.join(word_no_number_tokens))

#############################
#  Begin Yacc Declarations  #
#############################

#############################
#      Word lists, etc      #
#############################

def p_empty_1(p):
    'empty : '
    p[0] = []


def p_commas_2(p):
    'commas : COMMA'
    p[0] = p[1]


def p_commas_3(p):
    'commas : commas COMMA'
    p[0] = p[1]

def p_typed_word_4(p):
    'typed_word : word'
    p[0] = p[1]

def p_typed_word_5(p):
    'typed_word : word COLON word'
    p[0] = '%s:%s' % (p[1], p[3])


# Possibly empty list of words

def p_word_0_6(p):
    'word_0 : word'
    p[0] = p[1]


def p_word_0_7(p):
    'word_0 : word commas'
    p[0] = p[1]


# Possibly empty list of words or *

def p_word_list_word_0_list_8_9(p):
    'word_list_word_0_list_8 : '
    p[0] = []

def p_word_list_word_0_list_8_10(p):
    'word_list_word_0_list_8 : word_list_word_0_list_8 word_0'
    p[0] = p[1] + [p[2]]

def p_word_list_11(p):
    'word_list : word_list_word_0_list_8'
    p[0] = p[1]


def p_word_or_star_12(p):
    '''word_or_star : word
    | STAR'''
    p[0] = p[1]

def p_word_or_star_0_13(p):
    'word_or_star_0 : word_or_star'
    p[0] = p[1]


# Non-empty list of words

#nonempty_word_list: word_0 : $$ = [$1]
#                  : nonempty_word_list word_0 : $$ = $1 + [$2]

# Attribute lists contain specifications of the form ATTR=VALUE.  The
# return value is a list of (attribute, value) tupes.

def p_word_or_star_0_14(p):
    'word_or_star_0 : word_or_star commas'
    p[0] = p[1]

def p_attr_15(p):
    'attr : word EQUALS word'
    p[0] = (p[1], p[3])


def p_attr_0_16(p):
    'attr_0 : attr'
    p[0] = p[1]


def p_attr_0_17(p):
    'attr_0 : attr commas'
    p[0] = p[1]


def p_attr_list_attr_0_list_18_19(p):
    'attr_list_attr_0_list_18 : '
    p[0] = []

def p_attr_list_attr_0_list_18_20(p):
    'attr_list_attr_0_list_18 : attr_list_attr_0_list_18 attr_0'
    p[0] = p[1] + [p[2]]

def p_attr_list_21(p):
    'attr_list : attr_list_attr_0_list_18'
    p[0] = p[1]

def p_opt_paren_attr_list_22(p):
    'opt_paren_attr_list : empty'
    p[0] = p[1]

def p_opt_paren_attr_list_23(p):
    'opt_paren_attr_list : LPAREN attr_list RPAREN'
    p[0] = p[2]


# Extended attribute lists contain either VALUE or ATTR=VALUE.  The return
# value is a list of (attribute, value) tupes; when a bare value is given,
# the attribute is None.

def p_ext_attr_24(p):
    'ext_attr : word'
    p[0] = (None, p[1])

def p_ext_attr_25(p):
    'ext_attr : word EQUALS word'
    p[0] = (p[1], p[3])


def p_ext_attr_0_26(p):
    'ext_attr_0 : ext_attr'
    p[0] = p[1]


def p_ext_attr_0_27(p):
    'ext_attr_0 : ext_attr commas'
    p[0] = p[1]


def p_ext_attr_list_ext_attr_0_list_28_29(p):
    'ext_attr_list_ext_attr_0_list_28 : '
    p[0] = []

def p_ext_attr_list_ext_attr_0_list_28_30(p):
    'ext_attr_list_ext_attr_0_list_28 : ext_attr_list_ext_attr_0_list_28 ext_attr_0'
    p[0] = p[1] + [p[2]]

def p_ext_attr_list_31(p):
    'ext_attr_list : ext_attr_list_ext_attr_0_list_28'
    p[0] = p[1]

def p_opt_paren_ext_attr_list_32(p):
    'opt_paren_ext_attr_list : empty'
    p[0] = p[1]

def p_opt_paren_ext_attr_list_33(p):
    'opt_paren_ext_attr_list : LPAREN ext_attr_list RPAREN'
    p[0] = p[2]


#############################
#         Statements        #
#############################


def p_top_statement_list_34_35(p):
    'top_statement_list_34 : '
    p[0] = []

def p_top_statement_list_34_36(p):
    'top_statement_list_34 : top_statement_list_34 statement'
    p[0] = p[1] + [p[2]]

def p_top_37(p):
    'top : top_statement_list_34'
    p[0] = p[1]

def p_statement_38(p):
    '''statement : family_block
    | feature_block
    | rule_block
    | macro_def
    | word_block
    | testbed_block
    | relation_sorting_block'''
    p[0] = p[1]


#############################
#           Macros          #
#############################

def p_statement_39(p):
    'statement : SEMI'
    p[0] = p[1]


def init_macros():
    # Used to turn off macro substitution while processing a macro definition.
    global no_macro_sub
    no_macro_sub = 0

    # Needed to handle issue where macro def is immediately followed by
    # macro call.
    global return_bogus_value
    return_bogus_value = 0

    # Mapping of macro definitions to parameter list and text.
    global macro_defs
    macro_defs = {}

    # It doesn't really matter what the parameter names are for built-ins.
    # There just have to be the right number of them.
    macro_defs['regsub'] = MacroDef(['fromre', 'totext', 'str'], regsub)
    macro_defs['ifmatch'] = MacroDef(['regex', 'string', 'doif', 'doelse'],
                                     ifmatch)
    macro_defs['ifmatch-nocase'] = MacroDef(['regex', 'string', 'doif',
                                             'doelse'],
                                            ifmatch_nocase)

def save_macros(cur):
    cur.no_macro_sub = no_macro_sub
    cur.return_bogus_value = return_bogus_value
    cur.macro_defs = macro_defs

class MacroDef:
    def __init__(self, args, text):
        self.args = args
        self.text = text

class CCGToken(lex.LexToken):
    def __init__(self, type, value):
        self.type = type
        self.value = value

def arg_to_text(arg):
    return ''.join([str(x.value) for x in arg])

# Implementation of built-in 'regsub()': Concatenate the tokens into
# text, then do regex substitution.
def regsub(fromre, totext, string):
    return re.sub(arg_to_text(fromre), arg_to_text(totext),
                  arg_to_text(string))

# If REGEX matches the beginning of STRING, return DOIF, else return DOELSE.
def ifmatch(regex, string, doif, doelse):
    if re.match(arg_to_text(regex), arg_to_text(string)):
        return doif
    else:
        return doelse

# Same as ifmatch() but case-insensitive.
def ifmatch_nocase(regex, string, doif, doelse):
    if re.match(arg_to_text(regex), arg_to_text(string), re.IGNORECASE):
        return doif
    else:
        return doelse

def print_macros():
    for (key, value) in macro_defs.iteritems():
        print "Macro: %s(%s): %s" % (key, value.args, value.text)

# Given some text, expand the macros in it, recursively (i.e. apply
# any macros, then apply macros to the resulting text, etc.).  After
# that, combine text that has the . operator applied to it.
def macroexpand_text(text):
    if super_macro_debug:
        print "Text before expanding: %s" % arg_to_text(text)
    # Now recursively expand macros.  The code to actually check for
    # macros is in MacroLexer.
    lexer = MacroLexer(None)
    lexer.pushstack(text)
    newtext = []
    while True:
        tok = lexer.token()
        #print "Reading token: %s" % tok
        if not tok:
            break
        newtext.append(tok)
    text = newtext
    l = len(text)
    if super_macro_debug:
        print "Text after expanding: %s" % arg_to_text(text)
    # Now directly handle instances with the '.' operator, so that
    # the operator can be used to create new macro calls
    x = 1
    while x < l - 1:
        if (text[x].type == 'DOT' and text[x-1].type in ['ID', 'QUOTEDID']
            and text[x+1].type in ['ID', 'QUOTEDID']):
            tok = CCGToken(text[x-1].type,
                           text[x-1].value + text[x+1].value)
            tok.lineno = text[x].lineno
            # If either is quoted, the result should be quoted.
            if text[x+1].type == 'QUOTEDID':
                tok.type = 'QUOTEDID'
            text[x-1] = tok
            text[x:x+2] = []
            x -= 2
            l -= 2
        x += 1
    return text

# Return text of macro, with ARGS substituted for formal parameters of
# the macro.
def macrosub(macdef, args, lineno):
    text = macdef.text
    # If the text definition is a function (for builtins),
    # macro-expand the arguments, then call the function.
    if callable(text):
        args = [macroexpand_text(x) for x in args]
        text = text(*args)
        if type(text) is str:
            text = [CCGToken('QUOTEDID', text)]
            text[0].lineno = lineno
            return text
        else:
            return macroexpand_text(text)
    else:
        # Otherwise, make a copy of the text and substitute the arguments
        # into it.
        text = text[:]
        args = dict(zip(macdef.args, args))
        l = len(text)
        x = 0
        while x < l:
            if (text[x].type == 'ID' or text[x].type in directives) \
                   and text[x].value in args:
                newtext = args[text[x].value]
                text[x:x+1] = newtext
                l += len(newtext) - 1
                x += len(newtext) - 1
            x += 1
        return macroexpand_text(text)


# We need to do some hackery with BOGUS_VALUE in order to avoid problems
# when a macro definition is immediately followed by a call to that same
# macro.  The problem is that generally the parser wants to read one token
# ahead.  As a result, by the time it's processed the token that ends a
# macro definition, it's already read the following token -- and if that
# token begins a macro call, we're screwed.  To avoid this, we ensure that
# there is an extra BOGUS_VALUE token returned after every macro definition.
# To make this happen, we set a flag return_bogus_value just before the
# parser processes the token ending the macro definition.  At this point,
# the parser has already read that token from the lexer, and before it
# reduces that token, it reads the next token from the lexer -- which
# returns a bogus token, as we instructed it.


def p_macro_def_40(p):
    'macro_def : macro_def_1 BOGUS_VALUE'
    p[0] = p[1]

def p_turn_off_macro_sub_41(p):
    'turn_off_macro_sub : '
    global no_macro_sub
    no_macro_sub = 1


def p_return_bogus_value_42(p):
    'return_bogus_value : '
    global return_bogus_value
    return_bogus_value = 1
    global no_macro_sub
    no_macro_sub = 0


def p_macro_def_1_43(p):
    'macro_def_1 : turn_off_macro_sub DEF word LPAREN macro_param_list turn_on_linetrack RPAREN macro_text'
    macdef = MacroDef(p[5], p[8])
    macdef.args = p[5]
    macdef.text = p[8]
    if p[3] in macro_defs:
        error(p.lineno(0), "Redefining macro %s" % p[3])
    macro_defs[p[3]] = macdef
    #print_macros()



def p_macro_param_list_44(p):
    'macro_param_list : word_list'
    p[0] = p[1]


def p_macro_text_45(p):
    '''macro_text : bracemacro_text
    | linemacro_text'''
    p[0] = p[1]

def p_bracemacro_text_46(p):
    'bracemacro_text : turn_off_linetrack LBRACE bracemacro_text_list return_bogus_value RBRACE'
    p[0] = p[3]


def p_bracemacro_text_list_47(p):
    'bracemacro_text_list : empty'
    p[0] = []

def p_bracemacro_text_list_48(p):
    'bracemacro_text_list : bracemacro_text_list bracemacro_text_entry'
    p[0] = p[1] + p[2]


# The key thing about these is that they must be invoked BEFORE the
# token that tells you whether to turn the mode on or off.  If you
# try to set the global variable after (even directly after) the
# RPAREN or NEWLINE or whatever has been processed by a rule, it's too
# late: The parser has already looked ahead, and any newline directly
# following the token in question already processed the wrong way.
def p_turn_on_linetrack_49(p):
    'turn_on_linetrack : '
    global lexer_track_newlines
    lexer_track_newlines = 1


def p_turn_off_linetrack_50(p):
    'turn_off_linetrack : '
    global lexer_track_newlines
    lexer_track_newlines = 0



def p_bracemacro_text_entry(p):
    '''bracemacro_text_entry : LPAREN bracemacro_text_list RPAREN
                     | LBRACKET bracemacro_text_list RBRACKET
                     | LBRACE bracemacro_text_list RBRACE'''
    p[0] = [p.slice[1]] + p[2] + [p.slice[3]]

def p_bracemacro_text_entry_other(p):
    'FILLED IN BELOW'
    p[0] = [p.slice[1]]
# fill in the documentation (i.e. the cfg rule)
p_bracemacro_text_entry_other.func_doc = (
    'bracemacro_text_entry : ' + '\n| '.join(other_tokens + word_tokens)
    )


def p_linemacro_text_51(p):
    'linemacro_text : turn_off_linetrack return_bogus_value NEWLINE'
    p[0] = []


def p_linemacro_text_linemacro_next_list_52_53(p):
    'linemacro_text_linemacro_next_list_52 : '
    p[0] = []

def p_linemacro_text_linemacro_next_list_52_54(p):
    'linemacro_text_linemacro_next_list_52 : linemacro_text_linemacro_next_list_52 linemacro_next'
    p[0] = p[1] + [p[2]]

def p_linemacro_text_55(p):
    'linemacro_text : linemacro_begin linemacro_text_linemacro_next_list_52 turn_off_linetrack return_bogus_value NEWLINE'
    p[0] = [p[1]] + p[2]



def p_linemacro_begin(p):
    p[0] = p.slice[1]

def p_linemacro_next(p):
    p[0] = p.slice[1]

linemacro_begin_tokens = [x for x in tokens
                          if x != 'NEWLINE' and x != 'LBRACE']
linemacro_next_tokens = [x for x in tokens if x != 'NEWLINE']


# fill in the documentation (i.e. the cfg rule)
p_linemacro_begin.func_doc = (
    'linemacro_begin : ' + '\n| '.join(linemacro_begin_tokens)
    )
p_linemacro_next.func_doc = (
    'linemacro_next : ' + '\n| '.join(linemacro_next_tokens)
    )


#############################
#       Feature blocks      #
#############################


def init_features():
    # For each feature value, map its name to a CCGFeatval structure
    # describing it.
    global feature_values
    feature_values = {}
    # List of values for a particular feature; each value is a CCGFeatval,
    # listing a value, its parents, licensing info, macro info, and its
    # feature.
    global feature_to_values
    feature_to_values = {}
    # List of distributive features
    global distributive_features
    distributive_features = []
    # List of XML for licensing features
    global licensing_feature_xml
    licensing_feature_xml = []
    # Mapping of the names of feature values to the value inserted into the
    # XML 'val' attribute; usually the same as the name. (YUCK YUCK YUCK)
    global fv_names_to_values
    fv_names_to_values = {}

def save_features(cur):
    cur.feature_values = feature_values
    cur.feature_to_values = feature_to_values
    cur.distributive_features = distributive_features
    cur.licensing_feature_xml = licensing_feature_xml
    cur.fv_names_to_values = fv_names_to_values

# A feature value: The "name" of the feature value (corresponding to a
# feature macro), the parents of this value, and any licensing info.  Also
# may include a .feature, which is the "feature" that this value is a value
# for.
class CCGFeatval:
    def __init__(self, name, parents, licensing):
        self.name = name
        self.parents = parents
        self.licensing = licensing
    def __str__(self):
        return "CCGFeatval(%s, parents=%s, licensing=%s)" % (
            (self.name, self.parents, self.licensing))
    def __repr__(self):
        return str(self)

# Encapsulates directly obtained values and values obtained recursively,
# so we can avoid needlessly adding parents to the latter kind.
# Used temporarily when building the hierarchy.  Both direct and recursive
# are lists of CCGFeatvals.
class CCGFeatvalList:
    def __init__(self, direct, recursive=None):
        recursive = recursive or [] # fuckme!
        self.direct = direct
        self.recursive = recursive
    def __str__(self):
        return "CCGFeatvalList(direct=%s,recursive=%s)" % (self.direct,self.recursive)
    def __repr__(self):
        return str(self)

# For the given feature, and list of CCGFeatvals, convert the parents in
# each CCGFeatval to a list of CCGFeatvals rather just strings, and clean
# any excess parents.  Basically, if a value has multiple parents and one
# is reachable by following a path starting from another, it needs to be
# removed.

# FIXME!! Also output warnings when a featvar and featval have the same name.

def install_feature(feature, lis, lineno):
    # Add names to reverse-feature list and check for duplicates.
    for x in lis:
        if x.name in feature_values:
            warning(lineno, "Duplicate feature value `%s' (feature `%s', previously in feature `%s')",
                    x.name, feature, feature_values[x.name].feature)
        else:
            feature_values[x.name] = x
            x.feature = feature
    # Change the parents list of each value to point to actual featval objects
    # rather than just strings; check for unrecognized and duplicate values.
    for x in lis:
        newpar = []
        for y in x.parents:
            if y in feature_values:
                if feature_values[y] in newpar:
                    synerr("Duplicate feature value %s as parent of %s, feature %s",
                           y, x.name, feature)
                else:
                    newpar.append(feature_values[y])
            else:
                synerr("Unrecognized feature value %s as parent of %s, feature %s",
                       y, x.name, feature)
        x.parents = newpar

    # Check NODE and its parents to make sure it hasn't been seen before in
    # LIST, adding NODE to LIST as soon as it's seen.
    def check_cycles(node, list):
        if node in list:
            synerr("Cycle seen involving feature value %s", node.name)
        for x in node.parents:
            check_cycles(x, list + [node])
    
    # Check for cycles.
    for x in lis:
        check_cycles(x, [])

    # Check NODE and its parents to make sure that ORIGNODE is not reachable.
    def check_reachable(node, orignode):
        if node == orignode:
            return True
        for x in node.parents:
            if check_reachable(x, orignode):
                return True
        return False
    
    # Clean excess parents.
    for x in lis:
        newpar = []
        for y in x.parents:
            for z in x.parents:
                if z != y and check_reachable(z, y):
                    break
            else:
                newpar.append(y)
        x.parents = newpar

    # Finally: Add to feature list.
    feature_to_values[feature] = lis

# Return XML to go in types.xml.
def make_feature_types_xml():
    xml = []
    for (x, featvals) in feature_to_values.iteritems():
        # FIXME! Figure out what's going wrong here.
#        typename = x
#        print "fv_names_to_values: %s" % fv_names_to_values
#        if x in fv_names_to_values:
#            typename = fv_names_to_values[x]
#        xml += [['type', [('name', typename)]]]
        xml += [['type', [('name', x)]]]
        for y in featvals:
            if y.parents:
                xml += [['type',
                         [('name', y.name),
                          ('parents', ' '.join([z.name for z in y.parents]))]]]
            else:
                xml += [['type', [('name', y.name), ('parents', x)]]]
    return xml

# Return XML to go in morph.xml.
def make_feature_morph_xml():
    xml = []
    for x in my_sorted(feature_values):
        featval = feature_values[x]
        if featval.macrotie:
            entry = ['macro', [('name', '@%s' % x)]]
            for y in featval.macrotie:
                if type(y) is int:
                    entry += [['fs', [('id', y)],
                               ['feat',
                                [('attr', featval.feature),
                                 ('val', fv_names_to_values[x])]]]]
                else:
                    (wordtie, typename) = y
                    entry += [['lf', [],
                               ['satop', [('nomvar', wordtie)],
                                ['diamond', [('mode', typename)],
                                 ['prop',
                                  [('name', fv_names_to_values[x])]]]]]]
            xml += [entry]
    return xml

# Return XML to go in lexicon.xml.
def make_feature_lexicon_xml():
    xml = []
    if distributive_features:
        xml.append(['distributive-features',
                    [('attrs', ' '.join(distributive_features))]])
    if licensing_feature_xml:
        xml.append(['licensing-features', []] + licensing_feature_xml)
    return xml


# Allow you to override the value inserted by a feature macro, if you
# really want to (requested by Fred).

def p_featval_2_56(p):
    'featval_2 : word'
    p[0] = (p[1], p[1])

def p_featval_2_57(p):
    'featval_2 : word COLON word'
    p[0] = (p[1], p[3])


def p_featval_1_58(p):
    'featval_1 : featval_2'
    p[0] = p[1] + ([], [])

def p_featval_1_59(p):
    'featval_1 : featval_2 LBRACKET word_list RBRACKET'
    p[0] = p[1] + (p[3], [])

def p_featval_1_60(p):
    'featval_1 : featval_2 LPAREN attr_list RPAREN'
    p[0] = p[1] + ([], p[3])

def p_featval_1_61(p):
    'featval_1 : featval_2 LBRACKET word_list RBRACKET LPAREN attr_list RPAREN'
    p[0] = p[1] + (p[3], p[6])


def p_featval_62(p):
    'featval : featval_1'
    (name, value, parents, licensing) = p[1]
    fv_names_to_values[name] = value
    p[0] = CCGFeatval(name, parents, licensing)


def p_set_featval_63(p):
    'set_featval : featval'
    p[0] = CCGFeatvalList([p[1]])

def p_set_featval_64(p):
    'set_featval : featval LBRACE set_featval_list RBRACE'
    # The set_featval_list returns a CCGFeatvalList, where the direct entries
    # are those actually in the list itself, and the recursive entries
    # are descendants of them.  First add ourself as parent to the direct
    # entries.  Then move direct into recursive and put ourself as the only
    # direct entry.
    for x in p[3].direct:
        x.parents += [p[1].name]
    p[0] = CCGFeatvalList([p[1]], p[3].direct + p[3].recursive)


def p_set_featval_0_65(p):
    'set_featval_0 : set_featval'
    p[0] = p[1]


def p_set_featval_0_66(p):
    'set_featval_0 : set_featval commas'
    p[0] = p[1]

def p_set_featval_list_67(p):
    'set_featval_list : set_featval_0'
    p[0] = p[1]

def p_set_featval_list_68(p):
    'set_featval_list : set_featval_list set_featval_0'
    p[1].direct += p[2].direct; p[1].recursive += p[2].recursive; p[0] = p[1]


def p_featvar_69(p):
    'featvar : NUMBER'
    p[0] = int(p[1])

def p_featvar_70(p):
    'featvar : word_no_numbers'
    p[0] = (p[1], p[1])

def p_featvar_71(p):
    'featvar : word_no_numbers COLON word'
    p[0] = (p[1], p[3])


def p_featvar_0_72(p):
    'featvar_0 : featvar'
    p[0] = p[1]


def p_featvar_0_73(p):
    'featvar_0 : featvar commas'
    p[0] = p[1]

def p_opt_featspec_74(p):
    'opt_featspec : empty'
    p[0] = (None, None)

def p_opt_featspec_featvar_0_list_75_76(p):
    'opt_featspec_featvar_0_list_75 : featvar_0'
    p[0] = [p[1]]

def p_opt_featspec_featvar_0_list_75_77(p):
    'opt_featspec_featvar_0_list_75 : opt_featspec_featvar_0_list_75 featvar_0'
    p[0] = p[1] + [p[2]]

def p_opt_featspec_78(p):
    'opt_featspec : LESS opt_featspec_featvar_0_list_75 GREATER'
    p[0] = (p[2], None)

def p_opt_featspec_79(p):
    'opt_featspec : LPAREN attr_list RPAREN'
    p[0] = (None, p[2])

def p_opt_featspec_featvar_0_list_80_81(p):
    'opt_featspec_featvar_0_list_80 : featvar_0'
    p[0] = [p[1]]

def p_opt_featspec_featvar_0_list_80_82(p):
    'opt_featspec_featvar_0_list_80 : opt_featspec_featvar_0_list_80 featvar_0'
    p[0] = p[1] + [p[2]]

def p_opt_featspec_83(p):
    'opt_featspec : LESS opt_featspec_featvar_0_list_80 GREATER LPAREN attr_list RPAREN'
    p[0] = (p[2], p[5])



# We declare this in a slightly strange way to work around the awful bug
# involving non-recognition of empty RHS rules.
def p_opt_feature_bang_84(p):
    '''opt_feature_bang : empty
    | BANG'''
    p[0] = p[1]

def p_feature_decl_tail_85(p):
    'feature_decl_tail : SEMI'
    p[0] = []

def p_feature_decl_tail_86(p):
    'feature_decl_tail : COLON set_featval_list SEMI'
    p[0] = p[2]


def p_feature_decl_87(p):
    'feature_decl : opt_feature_bang word opt_featspec feature_decl_tail'
    if p[1]:
        global distributive_features
        distributive_features.append(p[2])
    if p[4]:
        feature_values = p[4].direct + p[4].recursive
    else:
        feature_values = []
    install_feature(p[2], feature_values, p.lineno(0))
    (macrotie, licensing) = p[3]
    # Add macro-tie info to each feature
    for x in feature_values:
        x.macrotie = macrotie
    # Handle licensing attributes on the feature values
    global licensing_feature_xml
    for x in feature_values:
        if x.licensing:
            licensing_feature_xml.append(
                ['feat',
                 [('attr', p[2]), ('val', fv_names_to_values[x.name])]
                 + x.licensing])
    # Handle licensing attributes on the feature itself rather than
    # on a feature value
    if licensing:
        licensing_feature_xml.append(['feat', [('attr', p[2])] + licensing])



#############################
#     Atomic categories     #
#############################

def p_feature_block_feature_decl_list_88_89(p):
    'feature_block_feature_decl_list_88 : '
    p[0] = []

def p_feature_block_feature_decl_list_88_90(p):
    'feature_block_feature_decl_list_88 : feature_block_feature_decl_list_88 feature_decl'
    p[0] = p[1] + [p[2]]

def p_feature_block_91(p):
    'feature_block : FEATURE LBRACE feature_block_feature_decl_list_88 RBRACE'
    p[0] = p[1]

def p_unification_id_92(p):
    'unification_id : NUMBER'
    p[0] = ('id', p[1])

def p_unification_id_93(p):
    'unification_id : TILDE NUMBER'
    p[0] = ('inheritsFrom', p[2])


def p_unification_id_0_94(p):
    'unification_id_0 : unification_id'
    p[0] = p[1]


def p_unification_id_0_95(p):
    'unification_id_0 : unification_id commas'
    p[0] = p[1]

def p_unification_id_spec_unification_id_0_list_96_97(p):
    'unification_id_spec_unification_id_0_list_96 : '
    p[0] = []

def p_unification_id_spec_unification_id_0_list_96_98(p):
    'unification_id_spec_unification_id_0_list_96 : unification_id_spec_unification_id_0_list_96 unification_id_0'
    p[0] = p[1] + [p[2]]

def p_unification_id_spec_99(p):
    'unification_id_spec : LESS unification_id_spec_unification_id_0_list_96 GREATER'
    p[0] = p[2]


def p_atomcat_bracket_entry_100(p):
    'atomcat_bracket_entry : word EQUALS word'
    p[0] = ['feat', [('attr', p[1]), ('val', p[3])]]


# FIXME!!!! Be more intelligent in determining how to separate nomvars
# and featvars, instead of just using some isupper() hacks.  We should
# check to see if the nomvars are represented in the corresponding
# hylo spec.  We should also output warnings if a bare word occurs and
# it is not identified anywhere as either a nomvar (should appear in hylo),
# a featvar (should appear in feature {}), or a featval (likewise).

def p_atomcat_bracket_entry_101(p):
    'atomcat_bracket_entry : word EQUALS STAR'
    p[0] = ['feat', [('attr', p[1]), ('val', '[*DEFAULT*]')]]


def p_atomcat_bracket_entry_102(p):
    'atomcat_bracket_entry : word'
    if p[1][0].isupper() and (len(p[1]) == 1 or not p[1][1].isupper()):
        p[0] = ['feat', [('attr', 'index')],
              ['lf', [], ['nomvar', [('name', p[1])]]]]
    elif p[1] in feature_values:
        p[0] = ['feat', [('attr', feature_values[p[1]].feature), ('val', p[1])]]
    else:
        p[0] = ['feat', [('attr', p[1])],
              ['featvar', [('name', "%s" % p[1].upper())]]]


def p_atomcat_bracket_entry_103(p):
    'atomcat_bracket_entry : word COLON word'
    p[0] = ['feat', [('attr', p[1])],
          ['featvar', [('name', "%s:%s" % (p[1].upper(), p[3]))]]]


def p_atomcat_bracket_entry_0_104(p):
    'atomcat_bracket_entry_0 : atomcat_bracket_entry'
    p[0] = p[1]


def p_atomcat_bracket_entry_0_105(p):
    'atomcat_bracket_entry_0 : atomcat_bracket_entry commas'
    p[0] = p[1]

def p_atomcat_bracket_atomcat_bracket_entry_0_list_106_107(p):
    'atomcat_bracket_atomcat_bracket_entry_0_list_106 : '
    p[0] = []

def p_atomcat_bracket_atomcat_bracket_entry_0_list_106_108(p):
    'atomcat_bracket_atomcat_bracket_entry_0_list_106 : atomcat_bracket_atomcat_bracket_entry_0_list_106 atomcat_bracket_entry_0'
    p[0] = p[1] + [p[2]]

def p_atomcat_bracket_109(p):
    'atomcat_bracket : LBRACKET atomcat_bracket_atomcat_bracket_entry_0_list_106 RBRACKET'
    p[0] = p[2]


# The use of word_except_x here is a hack to avoid a reduce/reduce conflict
# due to the use of x as an operator as well as a word.  Without it, the
# parser doesn't know, e.g., how to disambiguate something beginning
# FOO/x(... -- is x an operator or a category?  The parser only looks one
# character ahead, so it can't figure this out.  With this hack, you cannot
# use a single lowercase x as a category name without putting it in quotes,
# e.g. 'x'.

#############################
#           Slashes         #
#############################

def p_atomcat_110(p):
    'atomcat : word_except_x unification_id_spec atomcat_bracket'
    p[0] = ['atomcat', [('type', p[1])], ['fs', p[2]] + p[3]]


def p_atomcat_111(p):
    'atomcat : word_except_x unification_id_spec'
    p[0] = ['atomcat', [('type', p[1])], ['fs', p[2]] + []]


def p_atomcat_112(p):
    'atomcat : word_except_x  atomcat_bracket'
    p[0] = ['atomcat', [('type', p[1])], ['fs', []] + p[2]]


def p_atomcat_113(p):
    'atomcat : word_except_x'
    p[0] = ['atomcat', [('type', p[1])], ['fs', []] + []]


#  Temporary switch to Python mode to insert the needed function
slash_to_default_mode = {'/':'>', '\\':'<', '|':'.'}
ability_to_ability_value = {'@': 'active', '!': 'inert', None:None}
ability_value_to_ability = {'active': '@', 'inert': '!', None:None}
def makeslash(direc, mode, ability):
    if not mode:
        mode = slash_to_default_mode[direc]
    if direc == '|':
        direc = None
    ability = ability_to_ability_value[ability]
    return ['slash'] + [(direc and [('dir', direc)] or []) +
                        (ability and [('ability', ability)] or []) +
                        (mode and [('mode', mode)] or [])]


def p_bareslash_114(p):
    '''bareslash : SLASH
    | BACKSLASH
    | PIPE'''
    p[0] = p[1]


def p_slash_ability_115(p):
    '''slash_ability : AT
    | BANG'''
    p[0] = p[1]

def p_slash_mode_116(p):
    'slash_mode : X GREATER'
    p[0] = 'x>'

def p_slash_mode_117(p):
    'slash_mode : LESS X'
    p[0] = '<x'


def p_slash_mode_118(p):
    '''slash_mode : GREATER
    | LESS
    | X
    | DOT
    | STAR
    | CARET'''
    p[0] = p[1]

def p_slash_119(p):
    'slash : bareslash'
    p[0] = makeslash(p[1], None, None)

def p_slash_120(p):
    'slash : bareslash slash_mode'
    p[0] = makeslash(p[1], p[2], None)

def p_slash_121(p):
    'slash : bareslash slash_ability'
    p[0] = makeslash(p[1], None, p[2])

def p_slash_122(p):
    'slash : bareslash slash_mode slash_ability'
    p[0] = makeslash(p[1], p[2], p[3])

def p_slash_123(p):
    'slash : bareslash slash_ability slash_mode'
    p[0] = makeslash(p[1], p[3], p[2])


#############################
#     Complex categories    #
#############################

# Example:
# 
# Source:
# 
# s<1>[E] \ np<2>[X nom] / np<3>[Y acc]
#
#
# XML output:
# 
# <complexcat>
#   <atomcat type="s">
#     <fs id="1">
#       <feat attr="index">
#         <lf>
#           <nomvar name="E"/>
#         </lf>
#       </feat>
#     </fs>
#   </atomcat>
#   <slash dir="\" mode="&lt;"/>
#   <atomcat type="np">
#     <fs id="2">
#       <feat attr="index">
#         <lf>
#           <nomvar name="X"/>
#         </lf>
#       </feat>
#       <feat attr="case" val="nom"/>
#     </fs>
#   </atomcat>
#   <slash dir="/" mode="&gt;"/>
#   <atomcat type="np">
#     <fs id="3">
#       <feat attr="index">
#         <lf>
#           <nomvar name="Y"/>
#         </lf>
#       </feat>
#       <feat attr="case" val="acc"/>
#     </fs>
#   </atomcat>
# </complexcat>

######################

# This is a basic attempt to create a compact (BNF-style) syntax for
# representing legal XML for categories.  The idea is that this could be
# parsed and used to verify the XML, or perhaps to convert it to some other
# form.  It's not clear this is worth it -- there is already an XML schema
# notation for describing legal XML (albeit it's extremely obnoxious and
# verbose), and verifiers for verifying XML given a schema and a piece of
# XML, and XSLT (again, obnoxiously verbose) for transforming XML.

# category = ( atomcat | complexcat )

# complexcat {
#   atomcat
#   (slash (category | dollar) | dollar | setarg)+
#   lf?
# }

# basicArg = ( slash | category )
# dollarArg = ( slash | dollar )
# dollar(name)
# setarg { basicArg basicArg+ }

# atomcat(!type=[NMTOKEN]) {
#   fs?
#   lf?
# }
    
#   fs(id) {
#     (feat(attr='index') {
#        lf { nomvar(name) } }
#      | feat(attr, val)
#      )+
#   }

# slash(dir=('/', '\\', '|'), mode=('.', '*', '^', 'x', 'x<', '<x', '<', '>'),
#       varmodality, ability=('inert', 'active'))
      
# fs(id): 
#   ...


##################
# Initialization #
##################

# This maps face names to actual properties.  A face is the complete
# description for how a particular piece of text is to be displayed.  The
# properties can specify the font family, size, bold or not, italic or not,
# subscript or not, "scale" (modify the size by the specified value), and
# "inherit" to inherit from a specified face.  If not otherwise given,
# all faces inherit from the default.

face_mapping_init = {
    # The default mapping should contain a value for all parameters
    'default':{'family':'times', 'size':16, 'bold':False, 'italic':False,
               'subscript':False, 'superscript':False, 'scale':100,
               'foreground':None, 'background':None},
    'subscript':{'family':'helvetica', 'subscript':True, 'scale':70,
                 },
    'category':{'bold':True, 'family':'helvetica'},
    'dollar':{},
    'slash':{},
    'slash mode':{'subscript':True, 'scale':65},
    'paren':{},
    'brace':{},
    'family name':{'foreground':'blue', 'scale':130, 'bold':True},
    'lexical item':{'italic':True},
    'numeric index':{'inherit':'subscript'},
    'subscript comma':{'inherit':'subscript'},
    'nomvar':{'inherit':'subscript', 'foreground':'forest green'},
    'feature':{'inherit':'subscript', 'bold':True, 'foreground':'red'},
    # Ideally the following should be in small caps
    'caret':{'scale':115},
    'semname':{'scale':115},
    'semrole':{'scale':85, 'bold':True},
    'member heading':{},
    'member comma':{},
    'member':{'bold':True},
    }

# Offset to be applied to the baseline to handle subscripts and superscripts,
# relative to the size of the font of the subscripted/superscripted text.
# FIXME: Maybe should be relative to the larger size of the base (non-offset)
# text.  This would require that the 'scale' option not be handled in
# fixup_face_properties() but dealt with at the time that the offset is
# computed, so that the original text size is still available.
subscript_offset = -0.5
superscript_offset = 1

# Factor to scale all: FIXME, not currently working
zoom_factor = 100

# Merged and fixed up equivalent of the above.  This will also have a
# 'font' property containing the Tk font item corresponding to the
# family, size, bold, and italic properties.
face_mapping = {}

# Fix up a derived table of properties.  Currently this only handles 'scale'.
# This destructively modifies the property table.
def fixup_face_properties(props):
    if 'scale' in props:
        scale=props['scale']
        del props['scale']
        assert 'size' in props
        # Consider the size to its scaling factor
        props['size'] = int(props['size'] * scale / 100.0 + 0.5)
    family = props['family']
    size = props['size']
    weight = props['bold'] and 'bold' or 'normal'
    slant = props['italic'] and 'italic' or 'roman'
    props['font'] = tkFont.Font(family=family, size=size, weight=weight,
                                slant=slant)
    return props

# Merge two tables of properties, with P2 overriding P1.  Remove the
# 'inherit' property in the process.  Creates a new table, and does not
# modify P1 or P2.
def merge_face_properties(p1, p2):
    props = {}
    for x in p1:
        if x != 'inherit':
            props[x] = p1[x]
    for x in p2:
        if x != 'inherit':
            props[x] = p2[x]
    return props

# Derive the complete list of properties associated with a face name.
def face_properties(name):
    props = face_mapping_init[name]
    # If name is default, return properties directly
    if name == 'default':
        return merge_face_properties(props, {})
    # Else, determine where to inherit from and merge properties with
    # recursively computed value
    if 'inherit' in props:
        inherit = props['inherit']
    else:
        inherit = 'default'
    return merge_face_properties(face_properties(inherit), props)

# Compute the merged properties for all faces.
def compute_face_properties():
    for x in face_mapping_init:
        props = fixup_face_properties(face_properties(x))
        face_mapping[x] = props

def late_init_draw_once():
    compute_face_properties()

#################################
#   Drawing a section of text   #
#################################

# Create tags in a text widget corresponding to the faces and their
# properties. FIXME: Maybe we should do this only when needed, for each
# text widget.
def create_tags(text):
    for x in face_mapping:
        props = face_mapping[x]
        offs = 0
        if props['subscript']:
            offs = subscript_offset
        elif props['superscript']:
            offs = superscript_offset
        offs = offs*props['size']
        offs = '%sp' % offs # Dimension in points
        text.tag_config(x, font=props['font'], offset=offs)
        fg = props['foreground']
        bg = props['background']
        if fg:
            text.tag_config(x, foreground=fg)
        if bg:
            text.tag_config(x, background=bg)
        

# A "draw-into" object, used for incrementally building up some text
# in various fonts.  Initialized with a parent widget and some initial text.
# Drawing into it is done by calls to text().  When done, call finish()
# to return a widget containing the text (which can then be packed, gridded,
# etc.).
class draw_into(object):
    def __init__(self, master, width=120):
        self.wid = Text(master, height=3, width=width,
                        borderwidth=0, relief=FLAT,
                        background='white')
        self.curface = None
	self.wid.slash_image = []
        self.curtext = ''
        create_tags(self.wid)

	# Self.alltext maintains the length of the text printed 
	# for the current widget
	self.alltext = 0

	# FIXME: the tirgger for bigger height of the Text
	# widget is arbitrarily set to 95. This should be 
	# driven by width of individual fonts and chars
	self.expandTrigger = 95

    def finish_run(self):
        if self.curtext:
            self.wid.insert(INSERT, self.curtext, (self.curface,))
            #props = face_mapping[self.curface]
            #Label(self.wid, text=self.curtext,
            #      font=props['font']).pack(side=LEFT)
            self.curtext = ''
    def text(self, tex, face='default'):
        if self.curface == face:
            self.curtext += tex
        else:
            self.finish_run()
            self.curtext = tex
            self.curface = face

	# Increase recorded length of text
	self.alltext += len(tex)

	# Increase height if necessary
	if (self.alltext > self.expandTrigger):
		heightval = 3* (self.alltext/self.expandTrigger +1)
	    	self.wid.config(height= heightval)


    def finish(self):
        self.finish_run()
        self.wid.config(state=DISABLED)
        return self.wid
    def image(self, img):
	# When there is an image to be embedded
	self.finish_run()
	# Access the OPENCCG_HOME environment variable
	# to determine the correct path for the images
	openccg_home = os.environ['OPENCCG_HOME']
	gifdir = openccg_home+"/images/slashes/"
	image = PhotoImage(file=gifdir+img)
	# We are creating an instantiated variable here
	# for the image, because the actual photo object is destroyed once
	# the execution leaves the __init__ code. Without building it this way, 
	# the display was showing only a space for the image but not the image itself
	self.wid.slash_image += [image]
	self.wid.image_create(INSERT, image=image)

    def onHilite(self):
    	self.wid.config(bg = '#E9FFE3')

    def offHilite(self):
    	self.wid.config(bg = 'white')

def category_draw_children(into, chils, depth, vars, need_initial_comma=False,
                           sep='', sepface='default'):
    for x in chils:
        if sep and need_initial_comma:
            into.text(sep, sepface)
        need_initial_comma = True
        category_draw(into, x, depth=depth + 1, vars=vars)

# Given the XML for a category, draw a graphical representation into the
# widget INTO.  The drawing is done by calling into.text(TEXT, FACE)
def category_draw(into, xml, depth, vars):
    ty = xml[0]
    props = xml[1]
    chils = xml[2:]
    if ty == 'complexcat':
        if depth > 0:
            into.text('(', 'paren')
        category_draw_children(into, chils, depth, vars)
        if depth > 0:
            into.text(')', 'paren')
    elif ty == 'atomcat':
        into.text(getprop('type', props), 'category')
        category_draw_children(into, chils, depth, vars)
    elif ty == 'setarg':
        into.text('{', 'brace')
        category_draw_children(into, chils, depth, vars)
        into.text('}', 'brace')
    elif ty == 'fs':
        needcomma = False
        if vars.show_feat_id.get():
            idval = getoptprop('id', props)
            if idval:
                #into.text('<%s>' % idval, 'numeric index')
                into.text('%s' % idval, 'numeric index')
                needcomma = True
        if vars.show_feat_struct.get():
            category_draw_children(into, chils, depth, vars,
                                   need_initial_comma=needcomma, sep=',',
                                   sepface='subscript comma')
    elif ty == 'feat':
        attr = getprop('attr', props)
        if attr == 'index':
            assert len(chils) == 1
            assert chils[0][0] == 'lf'
            chils = chils[0][2:]
            assert len(chils) == 1
            assert chils[0][0] == 'nomvar'
            into.text(getprop('name', chils[0][1]), 'nomvar')
        else:
            val = getoptprop('val', props, None)
            if val:
                if vars.show_full_features.get():
                    into.text("%s=%s" % (attr, getprop('val', props)),
                              'feature')
                else:
                    into.text("%s" % getprop('val', props), 'feature')
            else:
                into.text("%s" % attr, 'feature')
    elif ty == 'slash':
        dir = getoptprop('dir', props, '|')
        mode = getoptprop('mode', props)
        ability = getoptprop('ability', props)
#        into.text('%s' % dir, 'slash')
#        into.text('%s%s' % (mode or '',
#                            ability_to_ability_value[ability] or ''),
#                  'slash mode')
 	
	# We create the file name here
	# By interpreting various parameters
	# and joiing them together as a string
	
	if dir == '\\':
		slash_string = 'bk'
	elif dir == '/':
		slash_string = 'fd'
	else:
		slash_string = 'str'

	#slash_mode : X GREATER : $$ = 'x>'
	#           : LESS X    : $$ = '<x'
	#           : GREATER | LESS | X | DOT | STAR | CARET
           
	modelist = {'x>':'cross_greater',
		    '<x':'lesser_cross',
		    '>':'greater',
		    '<':'lesser',
		    'x':'cross',
		    '.':'dot',
		    '*':'star',
		    '^':'box'}
	if mode == None:
		image_string = slash_string + '.GIF'
	else:
		image_string = slash_string+ '_' + modelist[mode] + '.GIF'
		
	into.image(image_string)

    elif ty == 'dollar':
        name = getoptprop('name', props)
        into.text('$', 'dollar')
        into.text('%s' % name, 'numeric index')
    else:
	# Have commented the following assert Statement
	# and the debug statement
	# Because of validation errors
        #debug('ty??? %s\n' % ty)
        #assert False
	dummy = 1


def p_complexcat_entry_124(p):
    'complexcat_entry : atomcat'
    p[0] = p[1]

def p_complexcat_entry_125(p):
    'complexcat_entry : LPAREN complexcat RPAREN'
    p[0] = p[2]


def p_complexcat_postmod_126(p):
    'complexcat_postmod : DOLLAR NUMBER'
    p[0] = [['slash', []], ['dollar', [('name', p[2])]]]


def p_complexcat_postmod_127(p):
    'complexcat_postmod : slash complexcat_entry'
    p[0] = [p[1], p[2]]


#complexcat_postmod : slash DOLLAR NUMBER :
#    $$ = [$1, ['dollar', [('name', $3)]]]

def p_complexcat_128(p):
    'complexcat : complexcat_entry'
    p[0] = p[1]

def p_complexcat_129(p):
    'complexcat : complexcat complexcat_postmod'
    if p[1][0] != 'complexcat':
        p[1] = ['complexcat', []] + [p[1]]
    p[0] = p[1] + p[2]


def p_cat_set_entry_130(p):
    'cat_set_entry : slash complexcat_entry'
    p[0] = [p[1], p[2]]


def p_cat_set_entry_0_131(p):
    'cat_set_entry_0 : cat_set_entry'
    p[0] = p[1]


def p_cat_set_entry_0_132(p):
    'cat_set_entry_0 : cat_set_entry commas'
    p[0] = p[1]

#############################
#       Hybrid logic        #
#############################

# Example:
# 
# Source:
# 
# E:action(* <Actor>X:animate-being <Patient>Y:sem-obj)
# 
# XML output:
# 
# <lf>
#   <satop nomvar="E:action">
#     <prop name="[*DEFAULT*]"/>
#     <diamond mode="Actor">
#       <nomvar name="X:animate-being"/>
#     </diamond>
#     <diamond mode="Patient">
#       <nomvar name="Y:sem-obj"/>
#     </diamond>
#   </satop>
# </lf>

def p_complexcat_cat_set_entry_0_list_133_134(p):
    'complexcat_cat_set_entry_0_list_133 : cat_set_entry_0'
    p[0] = [p[1]]

def p_complexcat_cat_set_entry_0_list_133_135(p):
    'complexcat_cat_set_entry_0_list_133 : complexcat_cat_set_entry_0_list_133 cat_set_entry_0'
    p[0] = p[1] + [p[2]]

def p_complexcat_136(p):
    'complexcat : complexcat LBRACE complexcat_cat_set_entry_0_list_133 RBRACE'
    # $3 comes as a list of lists of the form [slash, cat]; we need to
    # flatten the list, which is what reduce() does
    if p[1][0] != 'complexcat':
        p[1] = ['complexcat', []] + [p[1]]
    p[0] = p[1] + [['setarg', []] + reduce(lambda x,y:x+y, p[3])]



def hylo_draw_children(into, chils, need_initial_comma=False,
                       sep='', sepface='caret'):
    for x in chils:
        if sep and need_initial_comma:
            into.text(sep, sepface)
        need_initial_comma = True
        hylo_draw(into, x)

# Given the XML for a hylo, draw a graphical representation into the
# widget INTO.  The drawing is done by calling into.text(TEXT, FACE)
def hylo_draw(into, xml):
    ty = xml[0]
    props = xml[1]
    chils = xml[2:]
    if ty == 'satop':
        into.text('@')
        into.text(getprop('nomvar', props), 'nomvar')
        into.text('(')
        hylo_draw_children(into, chils, sep=' ^ ')
        into.text(')')
    elif ty == 'prop':
        name = getprop('name', props)
        if name == '[*DEFAULT*]':
            into.text('*', 'semname')
        else:
            into.text(name, 'semname')
    elif ty == 'diamond':
        mode = getprop('mode', props)
        # FIXME: Instead of uppercasing, we really want small caps
        into.text('<%s>' % mode.upper(), 'semrole')
        assert len(chils) > 0
        if len(chils) == 1:
            hylo_draw(into, chils[0])
        else:
            into.text('(')
            hylo_draw_children(into, chils, sep='^')
            into.text(')')
    elif ty == 'nomvar':
        into.text(getprop('name', props), 'nomvar')
    else:
        assert False

def p_hylo_entry_137(p):
    'hylo_entry : STAR'
    p[0] = ['prop', [('name', '[*DEFAULT*]')]]

def p_hylo_entry_138(p):
    'hylo_entry : typed_word'
    if p[1][0].isupper():
        p[0] = ['nomvar', [('name', p[1])]]
    else:
        p[0] = ['prop', [('name', p[1])]]


def p_hylo_entry_139(p):
    'hylo_entry : LESS word GREATER hylo_entry'
    p[0] = ['diamond', [('mode', p[2])], p[4]]


def p_hylo_entry_140(p):
    'hylo_entry : LESS word GREATER LPAREN hylo_list RPAREN'
    p[0] = ['diamond', [('mode', p[2])]] + p[5]


def p_carets_141(p):
    'carets : CARET'
    p[0] = p[1]


def p_carets_142(p):
    'carets : carets CARET'
    p[0] = p[1]

def p_hylo_entry_0_143(p):
    'hylo_entry_0 : hylo_entry'
    p[0] = p[1]


def p_hylo_entry_0_144(p):
    'hylo_entry_0 : hylo_entry carets'
    p[0] = p[1]

def p_hylo_list_145(p):
    'hylo_list : empty'
    p[0] = p[1]

def p_hylo_list_146(p):
    'hylo_list : hylo_list hylo_entry_0'
    p[0] = p[1] + [p[2]]


def p_hylo_list_0_147(p):
    'hylo_list_0 : hylo_list'
    p[0] = p[1]

def p_hylo_list_0_148(p):
    'hylo_list_0 : carets'
    p[0] = []

def p_hylo_list_0_149(p):
    'hylo_list_0 : carets hylo_list'
    p[0] = p[2]


def p_hylo_spec_150(p):
    'hylo_spec : typed_word LPAREN hylo_list_0 RPAREN'
    p[0] = ['satop', [('nomvar', p[1])]] + p[3]


def p_hybrid_logic_hylo_spec_list_151_152(p):
    'hybrid_logic_hylo_spec_list_151 : '
    p[0] = []

def p_hybrid_logic_hylo_spec_list_151_153(p):
    'hybrid_logic_hylo_spec_list_151 : hybrid_logic_hylo_spec_list_151 hylo_spec'
    p[0] = p[1] + [p[2]]

def p_hybrid_logic_154(p):
    'hybrid_logic : hybrid_logic_hylo_spec_list_151'
    p[0] = p[1]

def p_hybrid_logic_hylo_spec_list_155_156(p):
    'hybrid_logic_hylo_spec_list_155 : '
    p[0] = []

def p_hybrid_logic_hylo_spec_list_155_157(p):
    'hybrid_logic_hylo_spec_list_155 : hybrid_logic_hylo_spec_list_155 hylo_spec'
    p[0] = p[1] + [p[2]]

def p_hybrid_logic_158(p):
    'hybrid_logic : AT hybrid_logic_hylo_spec_list_155'
    p[0] = p[2]


#############################
#            Words          #
#############################


def init_morphology():
    global morph_xml
    morph_xml = []

    # List families/parts-of-speech of a word.  This comes from the
    # families/parts-of-speech specified in a word {} declaration; hence we
    # can't really tell families from POS's.  This also comes from any
    # member declarations inside of a family.
    global word_to_family_pos
    word_to_family_pos = {}
    # List word members of a family/part-of-speech; more or less the
    # inverse of the previous hash. (Not a perfect inverse because it
    # doesn't currently list any members that come from a member
    # declaration inside of a family, but only from word {} declarations.)
    global family_pos_to_word
    family_pos_to_word = {}
    # word->predicate mapping; this comes from pred=foo declarations in the
    # properties of a word.  This is needed because this info must be added
    # to <member> tags in a family.
    global word_to_predicate
    word_to_predicate = {}
    # Mapping of families to parts-of-speech; comes from family {}
    # declarations.
    global family_to_pos
    family_to_pos = {}
    # Contains a key for each part-of-speech seen in a family {}
    # declaration.
    global pos_hash
    pos_hash = {}
    # (XML for) list of word members explicitly specified using a member
    # statement.
    global family_members
    family_members = {}

def save_morphology(cur):
    cur.morph_xml = morph_xml
    cur.word_to_family_pos = word_to_family_pos
    cur.family_pos_to_word = family_pos_to_word
    cur.word_to_predicate = word_to_predicate
    cur.family_to_pos = family_to_pos
    cur.pos_hash = pos_hash
    cur.family_members = family_members

# Assume that hash[key] is a list, add VALUE to the list if not already there.
def add_uniquely_to_hash_entry_list(hash, key, value):
    if key not in hash:
        hash[key] = []
    if value not in hash[key]:
        hash[key] += [value]

def note_family_member(word, families):
    for x in families:
        add_uniquely_to_hash_entry_list(word_to_family_pos, word, x)
        add_uniquely_to_hash_entry_list(family_pos_to_word, x, word)

def make_word_morph_xml():
    xml = []
    for x in morph_xml:
        word_pos_list = []
        word = getprop('stem', x[1])
        # Each word needs to be listed as many times as it has parts of
        # speech.  We collect together all families and POS's associated
        # with a word, either from word {} or member declarations,
        # and determine all POS's from them.
        for y in word_to_family_pos.get(word, []):
            if y in family_to_pos:
                pos = family_to_pos[y]
            elif y in pos_hash:
                pos = y
            else:
                error(None, 'Family/part-of-speech %s not found (word declaration %s)',
                      y, word)
            if pos not in word_pos_list:
                word_pos_list += [pos]
        for y in word_pos_list:
            # Make a copy of the word's XML and set the POS appropriately.
            entry = x[:]
            putprop('pos', y, entry[1])
            xml += [entry]
    return xml


def p_word_param_159(p):
    'word_param : word_list'
    p[0] = (p[1], [])

def p_word_param_160(p):
    'word_param : word_list LPAREN ext_attr_list RPAREN'
    # WORD(VALUE) is equivalent to WORD(class=VALUE).
    property_name_replace(None, 'class', p[3])
    p[0] = (p[1], p[3])


def p_word_spec_1_161(p):
    'word_spec_1 : WORD word COLON word_param'
    (families, params) = p[4]
    note_family_member(p[2], families)
    pred = getoptprop('pred', params)
    if pred:
        word_to_predicate[p[2]] = pred
    p[0] = (p[2], [('pos', None), ('stem', p[2])] + params)


def p_word_spec_1_162(p):
    'word_spec_1 : WORD word COLON'
    p[0] = (p[2], [('pos', None), ('stem', p[2])])


def p_word_spec_163(p):
    'word_spec : WORD word'
    p[0] = (p[2], [('pos', None), ('stem', p[2])])


def p_word_spec_164(p):
    'word_spec : word_spec_1'
    p[0] = p[1]

def p_word_block_165(p):
    'word_block : word_spec SEMI'
    (word, params) = p[1]
    morph_xml.append(['entry', [('word', word)] + params])


def p_word_block_166(p):
    'word_block : word_spec_1 COLON word_macros SEMI'
    (word, params) = p[1]
    morph_xml.append(['entry', [('word', word)] + p[3] + params])


def p_word_macros_167(p):
    'word_macros : word_list'
    p[0] = [('macros', ' '.join(['@%s' % x for x in p[1]]))]


def p_word_form_168(p):
    'word_form : word_or_star SEMI'
    p[0] = (p[1], [])


def p_word_form_169(p):
    'word_form : word_or_star COLON word_macros SEMI'
    p[0] = (p[1], p[3])


def p_word_forms_170(p):
    'word_forms : '
    p[0] = []

def p_word_forms_171(p):
    'word_forms : word_forms word_form'
    p[0] = p[1] + [p[2]]


#############################
#       Family blocks       #
#############################

def p_word_block_172(p):
    'word_block : word_spec LBRACE word_forms RBRACE'
    (word, params) = p[1]
    for (form, macros) in p[3]:
        if form == '*':
            form = word
        morph_xml.append(['entry', [('word', form)] + macros + params])



def init_lexicon():
    global lexicon_xml
    lexicon_xml = []

def save_lexicon(cur):
    cur.lexicon_xml = lexicon_xml

# lexicon_xml already contains XML for each family and its entries (i.e.
# lexical insertion rules).  We also need to add to each family the words
# that are members of the family -- these come from both word {}
# declarations and member statements.
def make_family_lexicon_xml():
    for x in lexicon_xml:
        # Make sure that open families don't have member entries, or otherwise
        # [*DATE*], [*NUM*], etc. won't work.
        closed = getprop('closed', x[1])
        if closed == 'false':
            continue
        name = getprop('name', x[1])
        words_seen = []
        # Add each stem explicitly given in a member statement.  The
        # predicate comes from any predicate given in the member statement
        # along with the stem, or from the word {} declaration as a backup.
        for y in family_members[name]:
            stem = getprop('stem', y[1])
            words_seen += [stem]
            pred = getoptprop('pred', y[1])
            if not pred:
                pred = word_to_predicate.get(stem, None)
            x += [['member',
                   [('stem', stem)] + (pred and [('pred', pred)] or [])]]
        # Add each stem that specifies that it belongs to this family,
        # unless we already added it.
        for y in family_pos_to_word.get(name, []):
            if y not in words_seen:
                words_seen += [y]
                pred = word_to_predicate.get(y, None)
                x += [['member',
                       [('stem', y)] + (pred and [('pred', pred)] or [])]]
    return lexicon_xml


# A CSFamily is a `family {}' block.
class CSFamily(CSBlock):
    def __init__(self, prod, name, props, statements):
        super(CSFamily, self).__init__(prod)
        self.name = name
        self.props = props
        self.statements = statements
	self.text = None
	self.homeButton = None
	self.btnFrame = None
	self.menuHolder = None

	self.childFrame = None
	self.cfile = None
	self.cf = None
	self.vars = None
	self.canvas = None
	self.mainFrame = None

    def draw(self, childFrame, cfile, vars, row, canvas, mainFrame):
        # Draw the family name
        f = Frame(childFrame, bd=1, relief=SUNKEN, background='white')
        cf = draw_into(f, width=20)
        cf.text('%s' % self.name, 'family name')

	child_widget=cf.finish()
	self.menuHolder = child_widget

        child_widget.pack(fill=BOTH, expand=YES)

	child_widget.bind("<Button-1>", self.editPopup)

	self.childFrame = childFrame
	self.cfile = cfile
	self.cf = cf
	self.vars = vars
	self.canvas = canvas
	self.mainFrame = mainFrame


        f.grid(row=row, column=0, sticky=NSEW)

        # Draw the various statements
        f = Frame(childFrame, bd=1, relief=SUNKEN, background='white')
        for x in self.statements:
            frame = x.draw(f, cfile, vars)
            if frame:
                frame.pack(fill=BOTH, expand=YES)
        f.grid(row=row, column=1, sticky=NSEW)

        childFrame.rowconfigure(row, weight=1)

    # Define the binding procedure for the right-click for editing an entry
    def editPopup(self, event):
	popup = Menu(self.menuHolder, tearoff =0)
	popup.add_command(label=' Edit ', command = lambda: self.editSection(self.childFrame, 
				self.cfile, 
				self.cf, 
				self.vars, 
				self.canvas, 
				self.mainFrame))
	try:
		popup.tk_popup(event.x_root+40, event.y_root, 0)
	finally:
		popup.grab_release()
	
	# Now bind the right-click to the saveSection buttons
	self.menuHolder.bind("<Button-1>", self.savePopup)

    # Define the right click binding for the save entry
    def savePopup(self, event):
    	popup = Menu(self.menuHolder, tearoff = 0)
	popup.add_command(label = 'Done', command = lambda: self.saveSection(self.childFrame,
						self.cfile,
						self.cf,
						self.vars,
						self.canvas,
						self.mainFrame))
	popup.add_command(label = 'Home', command = lambda: self.editHome(self.cfile))

	fileData = self.cfile.getAllText()
	popup.add_command(label = 'Undo All', command = lambda: self.undoEdit(fileData, self.cfile))

	try:
		popup.tk_popup (event.x_root+40, event.y_root, 0)
	finally:
		popup.grab_release()
    
    # Edit a section, i.e. a family of the grammar individually rather than the entire grammar
    # Note that this will have very preliminary editing capabilities and the complete grammar
    # editing should be done through the Edit global view
    def editSection(self, childFrame, cfile, hiliteText, vars, canvas, mainFrame):
        editFrame = Frame(mainFrame, bd=1, background='white')

    	self.text = Text(editFrame, padx=5, wrap=None, undo = YES, background='white', height =10)
	vbar = Scrollbar(editFrame)
	hbar = Scrollbar(editFrame, orient='horizontal')

	self.text.config(yscrollcommand=vbar.set)    # call vbar.set on text move
        self.text.config(xscrollcommand=hbar.set)
        vbar.config(command=self.text.yview)         # call text.yview on scroll move
        hbar.config(command=self.text.xview)         # or hbar['command']=text.xview

	# Changing the mode of the cfile object here,
	# so that once the uer clicks done,
	# the whole object is recompiled and redisplayed
	cfile.mode= 'Edit'

	# Highlight the row being edited
	hiliteText.onHilite()

	vbar.pack(side=RIGHT, fill=Y)
	hbar.pack(side=BOTTOM, fill=X)
	self.text.pack(fill= BOTH, expand= YES)

	# Set a mark at the beginning of the text
	self.text.mark_set("START", INSERT)
	self.text.mark_gravity("START", LEFT)

	# Push in the rest of the file's contents
	fileData = cfile.getAllText()
	self.text.insert(INSERT, fileData)

	# Move the insert position to the first occurence of the family name
	# FIXME: this is poor implementation
	# The positioning of the insert cursor should be happening by parsing the 
	# CFG production rules, using CSFamily.prod.lineno and endlineno
	self.text.config(takefocus=True)
	idx= self.text.search('family '+ self.name, "START")
	self.text.mark_set(CURRENT, idx)
	self.text.see(CURRENT)

        #editFrame.grid(row=row+1, columnspan =3, sticky = NSEW)
        editFrame.grid(row=2, columnspan =2, sticky = NSEW)
	childFrame.update_idletasks()
	canvas.config(scrollregion=canvas.bbox("all"))

    # Finished editing
    #def saveSection(self, childFrame, cfile, hiliteText, varset, canvas, mainFrame, homeButton, undoButton):
    def saveSection(self, childFrame, cfile, hiliteText, varset, canvas, mainFrame):
    	# We force the text contents of the cfile object to copy over 
	# all that is presently in the current text-box
    	cfile.setAllText(self.text.get(1.0,END))

	# Undo the highlight of the row
	hiliteText.offHilite()

	# Recompile whatever was edited and redisplay
	# Note: changes are not saved hereby!!
	cfile.compile_if_needed()
	cfile.onLexicon()

	# Restore the right-click binding to the original
	self.menuHolder.bind("<Button-1>", self.editPopup)

    # Restore view to original place where you wanted to edit
    def editHome(self, cfile):
	# Move the insert position to the first occurence of the family name
	# FIXME: this is poor implementation
	# The positioning of the insert cursor should be happening by parsing the 
	# CFG production rules, using CSFamily.prod.lineno and endlineno
	self.text.config(takefocus=True)
	idx= self.text.search('family '+ self.name, "START")

	if not idx:
		showwarning('Error', 'Original entry for '+self.name+ ' not found!')
	self.text.mark_set(CURRENT, idx)
	self.text.see(CURRENT)

    # Undo all editing done till now
    def undoEdit(self, fileData, cfile):
    	askqn = askokcancel('Warning','Undo all changes till now?')
	if askqn:
		self.text.delete("START", END)
		self.text.insert(CURRENT, fileData)
		self.editHome(cfile)

    	

# CSFamilyEntry is an `entry' statement inside a `family' block.
#
# PROPS is a property list corresponding to the entry's name ('name') and
# any other properties, deriving from the form
#
# entry NAME(PROP=VAL, ...):
#
# Either the name or properties, or both, may be omitted.
#
# CAT is the XML corresponding to the entry's category, and LF is the XML for
# the logical form (hybrid logic).

class CSFamilyEntry(CSStatement):
    def __init__(self, prod, props, cat, lf=None):
        super(CSFamilyEntry, self).__init__(prod)
        self.props = props
        # NOTE: self.cat is a single XML statement, but self.lf is a list
        # of XML statements.  FIXME.
        self.cat = cat
        self.lf = lf
    def xml(self):
        if self.lf:
            lf = [['lf', []] + self.lf]
        else:
            lf = []
        return [['entry', self.props, self.cat + lf]]
    def draw(self, parent, cfile, vars):
        name = getoptprop('name', self.props)
        f = Frame(parent, background='white')
        cf = draw_into(f)
        cf.text('      ')
        if name:
            cf.text('%s: ' % name)
        category_draw(cf, self.cat, depth=0, vars=vars)
        if self.lf and vars.show_semantics.get():
            cf.text(' : ')
            hylo_draw_children(cf, self.lf)
        cf.finish().pack(fill=BOTH, expand=YES, side=LEFT)
        return f

# CSFamilyMember is a `member' statement inside a `family' block.  ITEMS
# lists the items given, in property-list form:
#
# STEM --> [('stem', STEM)]
# STEM(PRED) --> [('stem', STEM), ('pred', PRED)]

class CSFamilyMember(CSStatement):
    def __init__(self, prod, items):
        super(CSFamilyMember, self).__init__(prod)
        self.items = items
    def xml(self):
        return [['member', x] for x in self.items]
    def draw(self, parent, cfile, vars):
        return None
        f = Frame(parent, background='white', bd=1, relief=SUNKEN)
        cf = draw_into(f)
        cf.text('Members: ', 'member heading')
        first = True
        for x in self.items:
            stem = getprop('stem', x)
            pred = getoptprop('pred', x)
            if not first:
                cf.text(', ', 'member comma')
            cf.text(' %s%s' % (stem, pred and "(pred=%s)" % pred or ''),
                    'member')
            first = False

	print len (self.items) 
        cf.finish().pack(fill=BOTH, expand=YES)
        return f


# Omitting the colon between entry category and hybrid logic doesn't
# actually cause parsing problems, but it's probably not a good idea to
# encourage this, because the syntax might change in the future.


# We shouldn't need the first entry below, but we do, due to the bugginess
# in PLY in handling empty rules.
def p_entry_name_1_173(p):
    'entry_name_1 : opt_paren_attr_list'
    p[0] = p[1]

def p_entry_name_174(p):
    'entry_name : word'
    p[0] = [('name', p[1])]

def p_entry_name_175(p):
    'entry_name : word entry_name_1'
    p[0] = [('name', p[1])] + p[2]


def p_entry_name_176(p):
    'entry_name : entry_name_1'
    p[0] = p[1]

def p_entry_177(p):
    'entry : ENTRY entry_name COLON complexcat COLON hybrid_logic SEMI'
    p[0] = CSFamilyEntry(p, props=p[2], cat=p[4], lf=p[6])


def p_entry_178(p):
    'entry : ENTRY entry_name COLON complexcat SEMI'
    p[0] = CSFamilyEntry(p, props=p[2], cat=p[4])


def p_member_entry_179(p):
    'member_entry : word'
    p[0] = [('stem', p[1])]

def p_member_entry_180(p):
    'member_entry : word LPAREN word RPAREN'
    p[0] = [('stem', p[1]), ('pred', p[3])]


def p_member_entry_0_181(p):
    'member_entry_0 : member_entry'
    p[0] = p[1]


def p_member_entry_0_182(p):
    'member_entry_0 : member_entry commas'
    p[0] = p[1]

def p_member_member_entry_0_list_183_184(p):
    'member_member_entry_0_list_183 : member_entry_0'
    p[0] = [p[1]]

def p_member_member_entry_0_list_183_185(p):
    'member_member_entry_0_list_183 : member_member_entry_0_list_183 member_entry_0'
    p[0] = p[1] + [p[2]]

def p_member_186(p):
    'member : MEMBER COLON member_member_entry_0_list_183 SEMI'
    p[0] = CSFamilyMember(p, items=p[3])



def p_family_statement_187(p):
    '''family_statement : member
    | entry'''
    p[0] = p[1]

def p_family_statement_list_188(p):
    'family_statement_list : empty'
    p[0] = p[1]

def p_family_statement_list_189(p):
    'family_statement_list : family_statement_list family_statement'
    p[0] = p[1] + [p[2]]


#############################
#        Rule blocks        #
#############################

def p_family_block_190(p):
    'family_block : FAMILY word opt_paren_ext_attr_list LBRACE family_statement_list RBRACE'
    # FAMILY(VALUE) is equivalent to FAMILY(pos=VALUE).
    property_name_replace(None, 'pos', p[3])
    # Create the AST object -- before adding to $3.
    p[0] = CSFamily(p, name=p[2], props=p[3], statements=p[5])
    # 'pos' (part of speech) defaults to the family name; they would only
    # differ when more than one family is used to define a particular part of
    # speech, to handle related characteristics (family Prep-Nom vs. pos Prep).
    if not property_specified('pos', p[3]):
        p[3] += [('pos', p[2])]

    # Store mappings related to POS.
    pos = getprop('pos', p[3])
    family_to_pos[p[2]] = pos
    pos_hash[pos] = True

    # Now construct the XML for the family
    xml = ['family', [('name', p[2])] + p[3]]
    family_members[p[2]] = []
    for x in p[5]:
        if type(x) is CSFamilyMember:
            family_members[p[2]].extend(x.xml())
        else:
            xml.extend(x.xml())
    # If members have been specified ('member' statements) and there is no
    # 'closed' property, make the family closed.
    
    # if family_members[$2] and not property_specified('closed', xml[1]):
    #    xml[1] += [('closed', 'true')]
    
    # Actually, we *always* need classes closed, due to a bizarreness in
    # OpenCCG.
    if not property_specified('closed', xml[1]):
        xml[1] += [('closed', 'true')]
    # Add names to entries ('entry' statements) without them.
    primcount = 0
    for x in xml[2:]:
        if not property_specified('name', x[1]):
            primcount += 1
            x[1] = [('name', 'Entry-%s' % primcount)] + x[1]
    # For each specified member, note the family it's in so that its part
    # of speech can be calculated.
    for x in family_members[p[2]]:
        add_uniquely_to_hash_entry_list(word_to_family_pos,
                                        getprop('stem', x[1]),
                                        p[2])
    lexicon_xml.append(xml)
    p[0].static_xml = [xml]


def init_rules():
    global rules
    rules = {
        ('app', '+') : True,
        ('app', '-') : True,
        ('comp', '+') : True,
        ('comp', '-') : True,
        ('xcomp', '+') : True,
        ('xcomp', '-') : True,
        ('sub', '+') : False,
        ('sub', '-') : False,
        ('xsub', '+') : False,
        ('xsub', '-') : False,
        ('typeraise', '+') : [(False, True, True)],
        ('typeraise', '-') : [(True, True, True)],
        'typechange' : [],
        }

    global rules_to_xml_mapping
    rules_to_xml_mapping = {
        'app' : ['application', []],
        'comp' : ['composition', [('harmonic', 'true')]],
        'xcomp' : ['composition', [('harmonic', 'false')]],
        'sub' : ['substitution', [('harmonic', 'true')]],
        'xsub' : ['substitution', [('harmonic', 'false')]],
        }

def save_rules(cur):
    cur.rules = rules
    cur.rules_to_xml_mapping = rules_to_xml_mapping

def make_rules_xml():
    xml = []
    unique = 0
    for (key, value) in my_sorted(rules.items()):
        if type(key) is tuple and key[0] in rules_to_xml_mapping:
            rx = copy.deepcopy(rules_to_xml_mapping[key[0]])
            rx[1] += [('dir', key[1] == '+' and 'forward' or 'backward')]
            xml.append(rx)
        elif type(key) is tuple and key[0] == 'typeraise':
            for (dollar, arg, result) in value:
                xml.append(['typeraising',
                            [('dir', key[1] == '+' and 'forward'
                              or 'backward'),
                             ('useDollar', dollar and 'true' or 'false')]]
                           + (arg != True and
                              [['arg', [], arg]]
                              or [])
                           + (result != True and
                              [['result', [], result]]
                              or []))
        elif key == 'typechange':
            for (arg, result, lf) in value:
                unique += 1
        	if lf:
	            lf = [['lf', []] + lf]
	        else:
	            lf = []
                xml.append(['typechanging', [('name', 'typechange-%d' % unique)],
                            ['arg', [], arg],
                            ['result', [], result + lf]])
        else:
            raise InternalError("Invalid element in rules hash: %s" % str(key))
    return xml

def dotyperaise(plusminus, dollarp, arg, result):
    if plusminus == '+' or plusminus == '+-':
        rules[('typeraise', '+')] += [(dollarp, arg, result)]
    if plusminus == '-' or plusminus == '+-':
        rules[('typeraise', '-')] += [(dollarp, arg, result)]

def rulesreinit():
    rules.clear()
    rules[('typeraise', '+')] = []
    rules[('typeraise', '-')] = []
    rules['typechange'] = []


def p_ruletype_191(p):
    '''ruletype : APP
    | COMP
    | XCOMP
    | SUB
    | XSUB'''
    p[0] = p[1]

def p_opt_dollar_192(p):
    'opt_dollar : DOLLAR'
    p[0] = True

def p_opt_dollar_193(p):
    'opt_dollar : empty'
    p[0] = False


def p_opt_atomcat_194(p):
    'opt_atomcat : atomcat'
    p[0] = p[1]

def p_opt_atomcat_195(p):
    'opt_atomcat : empty'
    p[0] = True


def p_opt_complexcat_196(p):
    'opt_complexcat : COLON complexcat'
    p[0] = p[2]

def p_opt_complexcat_197(p):
    'opt_complexcat : empty'
    p[0] = True



def p_plusminus_spec_198(p):
    '''plusminus_spec : PLUS
    | MINUS
    | PLUSMINUS'''
    p[0] = p[1]

def p_rule_199(p):
    'rule : NO SEMI'
    rulesreinit()

def p_rule_200(p):
    '''rule : NO ruletype SEMI
    | NO ruletype PLUSMINUS SEMI'''
    del rules[(p[2], '+')]; del rules[(p[2], '-')]

def p_rule_201(p):
    'rule : NO ruletype PLUS SEMI'
    del rules[(p[2], '+')]

def p_rule_202(p):
    'rule : NO ruletype MINUS SEMI'
    del rules[(p[2], '-')]

def p_rule_203(p):
    '''rule : NO TYPERAISE SEMI
    | NO TYPERAISE PLUSMINUS SEMI'''
    rules[('typeraise', '+')] = []; rules[('typeraise', '-')] = []

def p_rule_204(p):
    'rule : NO TYPERAISE PLUS SEMI'
    rules[('typeraise', '+')] = []

def p_rule_205(p):
    'rule : NO TYPERAISE MINUS SEMI'
    rules[('typeraise', '-')] = []

def p_rule_206(p):
    'rule : NO TYPECHANGE SEMI'
    rules['typechange'] = []

def p_rule_207(p):
    'rule : ruletype PLUSMINUS SEMI'
    rules[(p[1], '+')] = True; rules[(p[1], '-')] = True

def p_rule_208(p):
    'rule : ruletype PLUS SEMI'
    rules[(p[1], '+')] = True

def p_rule_209(p):
    'rule : ruletype MINUS SEMI'
    rules[(p[1], '-')] = True

def p_rule_210(p):
    'rule : TYPERAISE plusminus_spec opt_dollar opt_complexcat SEMI'
    dotyperaise(p[2], p[3], p[4], True)

def p_rule_211(p):
    'rule : TYPERAISE plusminus_spec opt_dollar COLON complexcat GOESTO opt_atomcat SEMI'
    dotyperaise(p[2], p[3], p[5], p[7])

def p_rule_212(p):
    'rule : TYPECHANGE COLON complexcat GOESTO complexcat SEMI'
    rules['typechange'] += [(p[3], p[5], None)]

def p_rule_213(p):
    'rule : TYPECHANGE COLON complexcat GOESTO complexcat COLON hybrid_logic SEMI'
    rules['typechange'] += [(p[3], p[5], p[7])]


def p_rule_list_214(p):
    'rule_list : rule_list rule'
    p[0] = p[1]


def p_rule_list_215(p):
    'rule_list : empty'
    p[0] = p[1]


#############################
#           Testbed         #
#############################

def p_rule_block_216(p):
    'rule_block : RULE LBRACE rule_list RBRACE'
    p[0] = p[1]


def init_testbed():
    global testbed_statements
    testbed_statements = []

def save_testbed(cur):
    cur.testbed_statements = testbed_statements

def add_testbed_statement(bang, words, number):
    testbed_statements.append(['item',
                               [('string', ' '.join(words))] + bang +
                               number])

def make_testbed_xml():
    return testbed_statements


def p_opt_testbed_bang_217(p):
    'opt_testbed_bang : BANG'
    p[0] = [('known', 'true')]


def p_opt_testbed_bang_218(p):
    'opt_testbed_bang : empty'
    p[0] = p[1]

def p_testbed_entry_219(p):
    'testbed_entry : opt_testbed_bang word_list SEMI'
    add_testbed_statement(p[1], p[2], [])

def p_testbed_entry_220(p):
    'testbed_entry : opt_testbed_bang word_list COLON NUMBER SEMI'
    add_testbed_statement(p[1], p[2], [('numOfParses', p[4])])



#############################
#      Relation-sorting     #
#############################

def p_testbed_block_testbed_entry_list_221_222(p):
    'testbed_block_testbed_entry_list_221 : '
    p[0] = []

def p_testbed_block_testbed_entry_list_221_223(p):
    'testbed_block_testbed_entry_list_221 : testbed_block_testbed_entry_list_221 testbed_entry'
    p[0] = p[1] + [p[2]]

def p_testbed_block_224(p):
    'testbed_block : TESTBED LBRACE testbed_block_testbed_entry_list_221 RBRACE'
    p[0] = p[1]


def init_relation_sorting():
    global relation_sorting
    relation_sorting = []

def save_relation_sorting(cur):
    cur.relation_sorting = relation_sorting

def make_relation_sorting_lexicon_xml():
    if relation_sorting:
        return [['relation-sorting',
                 [('order', ' '.join(relation_sorting))]]]
    else:
        return []


#############################
#   End Yacc Declarations   #
#############################
def p_relation_sorting_block_word_or_star_0_list_225_226(p):
    'relation_sorting_block_word_or_star_0_list_225 : '
    p[0] = []

def p_relation_sorting_block_word_or_star_0_list_225_227(p):
    'relation_sorting_block_word_or_star_0_list_225 : relation_sorting_block_word_or_star_0_list_225 word_or_star_0'
    p[0] = p[1] + [p[2]]

def p_relation_sorting_block_228(p):
    'relation_sorting_block : RELATION_SORTING COLON relation_sorting_block_word_or_star_0_list_225 SEMI'
    global relation_sorting
    relation_sorting += p[3]



def p_error(p):
    if p:
        error(p.lineno, "Syntax error at '%s'", p.value)
    else:
        error(None, "Unexpected end of file")

#############################
#       Lexer classes       #
#############################

# A Lexer that allows for a list of tokens to be pushed onto the front of
# the list of tokens to be returned.  Any number of such lists can be
# pushed.
class StackLexer(object):
    def __init__(self, lexer):
        self.lexer = lexer
        self.tokenstack = []
        self.tokenstackind = []
        self.lineno = 1

    def input(self, s):
        self.lexer.input(s)

    def pushstack(self, stack):
        self.tokenstack.append(stack)
        self.tokenstackind.append(0)

    def token(self):
        global return_bogus_value
        if return_bogus_value:
            return_bogus_value = 0
            tok = CCGToken('BOGUS_VALUE', 'BOGUS_VALUE')
            tok.lineno = self.lineno
            return tok
        while self.tokenstack:
            try:
                tok = self.tokenstack[-1][self.tokenstackind[-1]]
                self.tokenstackind[-1] += 1
                return tok
            except IndexError:
                self.tokenstack.pop()
                self.tokenstackind.pop()
        if self.lexer:
            tok = self.lexer.token()
            if tok:
                self.lineno = tok.lineno
            return tok
        return None

# A Lexer that checks for macro calls and expands them appropriately.
class MacroLexer(StackLexer):
    def __init__(self, lexer):
        self.last_token = None
        self.indentlevel = 0
        super(MacroLexer, self).__init__(lexer)

    def simpletoken(self):
        return super(MacroLexer, self).token()

    def noeoftoken(self):
        tok = self.innertoken()
        if not tok:
            raise SyntaxError("Unexpected EOF")
        return tok
            
    def innertoken(self):
        macrotok = self.simpletoken()
        if not macrotok or no_macro_sub or \
               not (macrotok.type == 'ID' and macrotok.value in macro_defs):
            return macrotok
        else:
            newtok = self.simpletoken()
            if not newtok or newtok.type != 'LPAREN':
                self.pushstack([newtok])
                return macrotok
            macrodef = macro_defs[macrotok.value]
            args = []
            stop = False
            while not stop:
                thisarg = []
                parencount = 0
                expect_rbrace = 0
                newtok = self.noeoftoken()
                if newtok.type == 'LBRACE':
                    parencount += 1
                    expect_rbrace = 1
                    newtok = self.noeoftoken()
                while True:
                    if newtok.type in ['LBRACE', 'LBRACKET', 'LPAREN']:
                        parencount += 1
                    if newtok.type in ['RBRACE', 'RBRACKET', 'RPAREN']:
                        parencount -= 1
                    if parencount < 0:
                        if newtok.type == 'RPAREN':
                            stop = True
                            break
                        error(newtok.lineno, "Syntax error at %s",
                              newtok.value)
                        parencount = 0
                    if parencount == 0 and newtok.type == 'RBRACE' and \
                       expect_rbrace:
                        expect_rbrace = 0
                        newtok = self.noeoftoken()
                        continue
                    if parencount == 0 and newtok.type == 'COMMA':
                        break
                    thisarg.append(newtok)
                    newtok = self.noeoftoken()
                args.append(thisarg)
            # Allow extra trailing comma
            if len(args) == len(macrodef.args) + 1 and not args[-1]:
                args.pop()
            if len(args) != len(macrodef.args):
                error(macrotok.lineno,
                      "Invalid number of arguments to macro %s",
                      macrotok.value)
            else:
                if super_macro_debug:
                    print "Processing macro: %s" % macrotok.value
                self.pushstack(macrosub(macrodef, args, self.lineno))
                return self.innertoken()

    def token(self):
        def pretty_output_transformed(token):
            def newline(num=1):
                outout('\n' * num)
                outout(' ' * 2 * self.indentlevel)
            
            if tok.lineno and self.lineno < tok.lineno:
                if tok.lineno - self.lineno == 1:
                    newline()
                else:
                    newline(2)
            elif self.last_token and (self.last_token.type == 'RBRACE' or
                                      self.last_token.type == 'SEMI'):
                newline()
            elif tok.type == 'LBRACE':
                newline(2)
            value = str(tok.value)
            lastval = self.last_token and str(self.last_token.value)
            if value and lastval and ((isalnumund(lastval[0]) and
                                       isalnumund(value[0]))
                                      or self.last_token.type
                                      in ('COLON', 'COMMA')):
                outout(' ')
            if tok.type == 'QUOTEDID':
                outout('"%s"', value)
            else:
                outout('%s', value)
            if tok.type == 'LBRACE':
                self.indentlevel += 1
            elif tok.type == 'RBRACE':
                self.indentlevel -= 1
            return tok

        # Beginning of actual function
        tok = self.innertoken()
        if options.transformed_input and self.lexer and \
               tok and tok.type != 'BOGUS_VALUE':
               pretty_output_transformed(tok)
        self.last_token = tok
        # print "Saw token: %s" % tok
        return tok

#############################
#           Parsing         #
#############################

def init_parse_once():
    # Initialize the parser once, at beginning.  This does introspection on
    # the rules (i.e. p_*() functions) in this file.
    yacc.yacc(start='top', debug=yacc_debug, method='LALR', write_tables=0)

# Parse a .CCG file whose contents are in STR.

class parse_results:
    pass

def parse_string(str):
    retval = parse_results()
    if str:
        retval.parse = yacc.parse(str, lexer=MacroLexer(globallexer))
    else:
        retval.parse = []
    save_global_state(retval)
    return retval

#############################
#           Graphics        #
#############################

# Given the return value from parsing (a list of abstract syntax tree-related
# objects), draw them into the given frame.

def draw_parse(parse, cfile, childFrame, vars, canvas, mainFrame):
    row = 0

    if parse:
    	for x in parse:
        	if hasattr(x, 'draw'):
            		x.draw(childFrame, cfile, vars, row, canvas, mainFrame)
            		row += 1
    	# Make the column containing the lexical entries expand as necessary
    	childFrame.columnconfigure(1, weight=1)
    	#frame.grid(column=0)



#############################
#       Initialization      #
#############################

# We encapsulate all global-variable initialization into a function that
# can be called repeatedly so we can reinitialize our state and parse more
# than one file.  ARGV is the command-line arguments to parse (normally
# sys.argv[1:]) and ERRORS_TO_STRING indicates whether to write stdout and
# stderr output to strings or to the normal output locations.

def init_global_state(errors_to_string=False):
    init_errors(errors_to_string)
    init_lexer()
    init_macros()
    init_features()
    init_morphology()
    init_lexicon()
    init_testbed()
    init_rules()
    init_relation_sorting()

# When we're finished parsing, save the global state to the specified
# object, so we can track the parse results for more than one file.
def save_global_state(cur):
    save_errors(cur)
    save_lexer(cur)
    save_macros(cur)
    save_features(cur)
    save_morphology(cur)
    save_lexicon(cur)
    save_testbed(cur)
    save_rules(cur)
    save_relation_sorting(cur)

def init_global_state_once():
    init_parse_once()

late_init_graphics_done = 0
# Graphics-related initialization that must be done late, after the first
# Tk top-level window has been created.
def late_init_graphics():
    global late_init_graphics_done
    if not late_init_graphics_done:
        late_init_draw_once()
        late_init_graphics_done = 1
    

#############################
#         Main driver       #
#############################

# Function to output a particular XML file
def output_xml_file(prefix, grammar_name, filebase, top_level_tag, xml):
    xml_file = os.path.join(options.dir, '%s%s.xml' % (prefix, filebase))
    if not options.quiet:
        errout('Outputting XML file: %s\n' % xml_file)
    xml = [top_level_tag, [('name', grammar_name),
                           ('xmlns:xsi',
                            'http://www.w3.org/2001/XMLSchema-instance'),
                           ('xsi:noNamespaceSchemaLocation',
                            '../%s.xsd' % filebase)]] + xml
    fil = open(xml_file, 'w')
    fil.write('<?xml version="1.0" encoding="UTF-8"?>\n')
    print_xml(fil, xml)
    fil.close()

def make_grammar_xml(prefix):
    return [['lexicon', [('file', '%slexicon.xml' % prefix)]],
            ['morphology', [('file', '%smorph.xml' % prefix)]],
            ['rules', [('file', '%srules.xml' % prefix)]],
            ['types', [('file', '%stypes.xml' % prefix)]]]

# Map saying how to output the specified XML file
output_file_map = {
    'lexicon': ('ccg-lexicon',
                lambda pref: make_feature_lexicon_xml() +
                make_relation_sorting_lexicon_xml() +
                make_family_lexicon_xml()),
    'rules': ('rules', lambda pref: make_rules_xml()),
    'morph': ('morph',
              lambda pref: make_word_morph_xml() + make_feature_morph_xml()),
    'types': ('types', lambda pref: make_feature_types_xml()),
    'grammar': ('grammar', make_grammar_xml),
    'testbed': ('regression', lambda pref: make_testbed_xml()),
    }

# Process the --omit-output list.

def split_output_files(arg):
    files = re.split('[,\s]+', arg)
    for x in files:
        if x not in output_file_map:
            parser.error('Unknown file in --omit-output argument')
    return files

def main():
    parse_arguments(sys.argv[1:])
    init_global_state_once()
    init_global_state()

    if options.omit_output:
        if options.omit_output[0] == '+':
            output_files = split_output_files(options.omit_output[1:])
        else:
            suppress_output = split_output_files(options.omit_output)
            output_files = []
            for x in output_file_map:
                if x not in suppress_output:
                    output_files.append(x)
    else:
        output_files = [x for x in output_file_map]
    
    # Now actually parse the input arguments

    prefix = options.prefix
    lastfile = '-'
    args = global_args or ['-']
    
    for arg in args:
        if arg == '-':
            if not options.quiet:
                errout("ccg2xml: Processing standard input\n")
            fil = sys.stdin
        else:
            if not options.quiet:
                errout("ccg2xml: Processing %s\n" % arg)
            fil = file(arg)
            lastfile = arg
            if prefix == None:
                (phead, ptail) = os.path.split(arg)
                (pbase, pext) = os.path.splitext(ptail)
                prefix = '%s-' % pbase
        retval = parse_string(fil.read())
        # print "Retval: %s\n" % retval
    
    if macro_debug:
        print_macros()
    
    # Make output directory if needed, and output files

    if error_count > 0:
        if not options.quiet:
            maybe_errout('Errors during compilation, files not output.\n')
        sys.exit(1)
    else:
        if options.dir:
            if not os.path.isdir(options.dir):
                os.makedirs(options.dir)
        else:
            options.dir = '.'
        
        for x in output_files:
            file_info = output_file_map[x]
            output_xml_file(prefix, lastfile, x, file_info[0],
                            file_info[1](prefix))

if __name__ == '__main__':      # when run as a script
    main()

# Local Variables:
# mode: python
# end:
